{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "creative-harrison",
   "metadata": {},
   "source": [
    "# Data Mining - Handin 3 - Pattern mining\n",
    "\n",
    "This handin corresponds to the topics in Week 15--18 of the course.\n",
    "\n",
    "The handin is\n",
    "* done in groups\n",
    "* worth 10% of the grade\n",
    "\n",
    "For the handin, you will prepare a report in PDF format, by exporting the Jupyter notebook. Please submit the following no later than **May 16th 23:59 CET**.\n",
    "1. The Jupyter notebook file with your answers\n",
    "2. The PDF obtained by exporting the Jupyter notebook\n",
    "\n",
    "**The grading system**: Tasks are assigned a number of points based on the difficulty and time to solve it. The sum of the number of points is **70**. For the maximum grade you need to get at least _63 points_. The minimum grade (02 in the Danish scale) requires **at least** 21 points, with at least 8 points from Parts 1 and 2, and 5 points from Part 3. Good luck!\n",
    "\n",
    "**The exercise types**: There are five different types of exercises\n",
    "1. <span style='color: green'>**\\[Compute by hand\\]**</span> means that you should provide NO code, but show the main steps to reach the result (not all). \n",
    "2. <span style='color: green'>**\\[Motivate\\]**</span> means to provide a short answer of 1-2 lines indicating the main reasoning, e.g., the PageRank of a complete graph is 1/n in all nodes as all nodes are symmetric and are connected one another.\n",
    "3. <span style='color: green'>**\\[Describe\\]**</span> means to provide a potentially longer answer of 1-5 lines indicating the analysis of the data and the results.\n",
    "4. <span style='color: green'>**\\[Prove\\]**</span> means to provide a formal argument and NO code.\n",
    "5. <span style='color: green'>**\\[Implement\\]**</span> means to provide an implementation. Unless otherwise specified, you are allowed to use helper functions (e.g., ```np.mean```, ```itertools.combinations```, and so on). However, if the task is to implement an algorithm, by no means a call to a library that implements the same algorithm will be deemed as sufficient!\n",
    "\n",
    "**Q&A**\n",
    "\n",
    "Q: If the task is to implement a mean function, may I just call ```np.mean()```? \n",
    "<br>A: No.\n",
    "\n",
    "Q: If the task is to compare the mean of X and Y, may I use ```np.mean()``` to calculate the mean?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: If I have implemented a mean function in a previous task, but I am unsure of its correctness, may I use ```np.mean()``` in following task where mean is used as a helper function? \n",
    "<br>A: Yes.\n",
    "\n",
    "Q: May I use ```np.mean()``` to debug my implementation of mean?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Do I get 0 points for a task if I skip it?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Can I get partial points for a task I did partially correct?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Is it OK to skip a task if I do not need the points from it?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Should I inform a TA if I find an error?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Should I ask questions if I am confused?\n",
    "<br>A: Yes.\n",
    "\n",
    "\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd973f4",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">New packages have been added to \"requirements.yml\". To install them, go to the root of the repository and run:\n",
    "```\n",
    "conda activate dm25\n",
    "conda env update -f requirements.yml --prune\n",
    "```\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-shakespeare",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'dm25 (Python 3.8.20)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. WebSocket is not defined"
     ]
    }
   ],
   "source": [
    "### BEGIN IMPORTS - DO NOT TOUCH!\n",
    "\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tabulate\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse\n",
    "import faiss\n",
    "import unicodedata\n",
    "\n",
    "from datasets import load_dataset\n",
    "from utilities.load_data import load_market_basket\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.custom_metrics import top_k_accuracy\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utilities')\n",
    "#from load_data import load_dblp_citations\n",
    "\n",
    "### END IMPORTS - DO NOT TOUCH!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-cornell",
   "metadata": {},
   "source": [
    "# Part 1: Subgraph mining (25 Points)\n",
    "In this part, we will work with subgraph mining algorithms. We will first solve some theory exercises and then implement two simple algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-softball",
   "metadata": {},
   "source": [
    "## Task 1.1 DFS codes (13 Points)\n",
    "\n",
    "### Task 1.1.1 (6 Points)\n",
    "<span style='color: green'>**\\[Compute by hand\\]**</span> Find the canonical (i.e., minimal) DFS code for the graph below. Try to eliminate some codes without generating the complete search tree. *Hint*: you can eliminate a code if you can show that it will have a larger code than some other code (e.g., using label ordering, degree). \n",
    "\n",
    "<div>\n",
    "<img src=\"data/dfs-codes.png\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-surprise",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-budget",
   "metadata": {},
   "source": [
    "### Task 1.1.2 (4 Points)\n",
    "<span style='color: green'>**\\[Describe\\]**</span> an extension to the DFS-code notation and the rules for the lexicographic ordering that handles the case of *directed* graphs. If that is not possible, state why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-rapid",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-correction",
   "metadata": {},
   "source": [
    "### Task 1.1.3 (3 Points)\n",
    " <span style='color: green'>**\\[Describe\\]**</span> (no need for pseudocode) a suitable way to find the _maximum_ DFS-code from the rules for _minimum_ DFS-codes that you already know from the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-exhibit",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-creation",
   "metadata": {},
   "source": [
    "## Task 1.2 Maximum Independent Set (12 Points)\n",
    "\n",
    "### Task 1.2.1 (6 Points)\n",
    "<span style='color: green'>**\\[Prove\\]**</span>  Sketch a proof that the Maximum Independent Set (MIS) support is anti-monotone, i.e., the support of a pattern $P'$ is no larger than _any_ pattern $P$ included in $P'$ (that is, $P$ is a sub-pattern of $P'$). To guide you into the proof, start from a set of matchings of the pattern $P'$ which corresponds to an independent set of nodes $I'$ in the overlap graph $G'_{O}$, same for the set of nodes $I$ in the overlap graph $G_O$ of $P$. Observe (_Observation 1_) that the **all** the matchings $f'$ of $P'$ contain matchings $f$ of $P$. Also observe (_Observation 2_) that if you take two matchings $f_1'$ and $f_2'$ of $P'$ and the corresponding matchings $f_1$ and $f_2$ of $P$ overlap, so do the matchings $f_1'$ and $f_2'$. Given these two observation what can you deduce on the independent sets $I'$ of $G'_O$ and $I$ of $G_O$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-graduation",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-reynolds",
   "metadata": {},
   "source": [
    "### Task 1.2.2 (6 Points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> In this exercise, you will program a simplified version of the Maximum Independent Set (MIS) support. Your exercise is to construct an algorithm that takes as input a pattern $P$ and the matches of the pattern in the graph $G$, and finds the Maximum Independent Set (MIS) support. Since finding the MIS is NP-hard, your exercise is to implement a simple greedy approximation algorithm. To test the code, you can use the graph and code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mis_support(P, matches): \n",
    "    \"\"\"\n",
    "    Returns the MIS support of a pattern. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    P:       The pattern represented as a networkx undirected graph object\n",
    "    matches: A list of subgraph isomorphic matches. Each match is a dictionary id_node_pattern -> id_node_graph\n",
    "    \"\"\"\n",
    "    mis = 0\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import isomorphism\n",
    "\n",
    "# Example pattern\n",
    "P = nx.Graph()\n",
    "P.add_nodes_from([(1,{\"label\":\"A\"}), (2,{\"label\":\"B\"}), (3,{\"label\":\"C\"})])\n",
    "P.add_edges_from([(1,2),(2,3)])\n",
    "labels = nx.get_node_attributes(P, 'label') \n",
    "plt.figure(1)\n",
    "nx.draw(P,labels=labels)\n",
    "\n",
    "# Example graph\n",
    "G = nx.read_gml(\"data/graph.gml\", label='id')\n",
    "labels = nx.get_node_attributes(G, 'label') \n",
    "pos = nx.spring_layout(G)\n",
    "plt.figure(2)\n",
    "nx.draw(G,pos, labels=labels)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Find the matches of P in G\n",
    "nm = isomorphism.GraphMatcher(G,P,node_match=isomorphism.categorical_node_match(\"label\", \"A\"))\n",
    "matches = []\n",
    "for subgraph in nm.subgraph_monomorphisms_iter():\n",
    "    matches.append(subgraph)\n",
    "    print(subgraph)\n",
    "    \n",
    "print(\"The MIS support for pattern %s in G is: %f\" %(P.nodes, mis_support(P, matches)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXTRA CODE BLOCK HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-evolution",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-contact",
   "metadata": {},
   "source": [
    "# Part 2: Frequent itemsets (25 Points)\n",
    "We have learned the Apriori and FP-Growth algorithms for mining frequent itemsets. In this part, we will implement these algorithms and compare their performance.\n",
    "\n",
    "We will use the anonymized real-world Kosarak clickstream dataset from: http://fimi.ua.ac.be/data/.\n",
    "This dataset was collected from a Hungarian online news portal, where each transaction represents a single user session (i.e., the list of pages a user visited). It contains approximately 990,000 transactions and over 41,000 unique items. \n",
    "\n",
    "You may work with the top-50 most frequent items to reduce runtime and improve clarity.\n",
    "_Hint:_ Each transaction contains a space-separated list of integer item IDs.\n",
    "\n",
    "After running the code block below, the variable **kosarak_small** will contain the filtered transactions with top-50 frequent items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.load_data import load_market_basket\n",
    "\n",
    "def filter_transactions(T, k=50):\n",
    "    \"\"\"\n",
    "    Keep only the top k items in the transactions.\n",
    "    Remove transactions that become empty.\n",
    "    \"\"\"\n",
    "    # Find the max item ID dynamically\n",
    "    max_item_id = max(i for t in T for i in t) + 1\n",
    "    counts = [0] * max_item_id\n",
    "\n",
    "    for t in T:\n",
    "        for i in t:\n",
    "            counts[i] += 1\n",
    "\n",
    "    # Sort and select top k\n",
    "    counts = np.array(counts)\n",
    "    order  = np.argsort(counts)[::-1] # reverse the sorted order\n",
    "\n",
    "    indexes_to_keep = order[:k]  # Keep the top k items\n",
    "    index_set = set(indexes_to_keep)   # Convert to python set for efficiency\n",
    "\n",
    "    # Filter transactions\n",
    "    T_new = [t_ for t_ in [list(filter(lambda i: i in index_set, t)) for t in T] if t_]\n",
    "    return T_new\n",
    "\n",
    "\n",
    "kosarak = load_market_basket()\n",
    "kosarak_small = filter_transactions(kosarak)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-psychology",
   "metadata": {},
   "source": [
    "## Task 2.1 Association Rules (4 Points)\n",
    "Consider the following table\n",
    "\n",
    "| Transaction ID | Items              |\n",
    "|----------------|--------------------|\n",
    "| 1              | Ape, Cat, Dog, Cow |\n",
    "| 2              | Cat, Dog, Pig, Cow |\n",
    "| 3              | Dog, Bat, Pig, Cow |\n",
    "| 4              | Dog, Pig, Cow      |\n",
    "| 5              | Dog, Cow           |\n",
    "| 6              | Cat, Cow           |\n",
    "| 7              | Ape, Bat, Fox      |\n",
    "| 8              | Ape, Cow           |\n",
    "| 9              | Ape, Dog, Cow      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-shade",
   "metadata": {},
   "source": [
    "### Task 2.1.1 (0.5 Points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> What is the count of the itemset {Dog,Pig,Cow} ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-account",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-adolescent",
   "metadata": {},
   "source": [
    "### Task 2.1.2 (0.5 Points)\n",
    "<span style='color: green'>**\\[Compute by hand\\]**</span> What is the support and confidence of the association rule {Dog,Pig}->Cow ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-kennedy",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-seattle",
   "metadata": {},
   "source": [
    "### Task 2.1.3 (1.5 Point)\n",
    "<span style='color: green'>**\\[Compute by hand\\]**</span> Consider the application of the Apriori algorithm to find all the frequent itemsets\n",
    "whose counts are at least 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-prime",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-walnut",
   "metadata": {},
   "source": [
    "### Task 2.1.4 (1.5 Point)\n",
    "<span style='color: green'>**\\[Compute by hand\\]**</span> Find all the association rules with support at least 1/3 and confidence at least 1/2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-wright",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-transformation",
   "metadata": {},
   "source": [
    "## Task 2.2 A Priori algorithm (9 Points)\n",
    "\n",
    "### Task 2.2.1 (7 Points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> Develop an implementation of the Apriori algorithm. You can look at your implementation from the exercises (note that this one is slightly different to simplify comparison with FP-Growth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_algorithm(T, min_support=10):\n",
    "    \"\"\"\n",
    "        Apriori algorithm for mining frequent itemsets and association rules. \n",
    "        This implementation should just find frequent itemsets, and ignore the rule generation.\n",
    "        Inputs:\n",
    "            T:               A list of lists, each inner list will contiain integer-item-ids. \n",
    "                             Example: T = [[1, 2, 5], [2, 3, 4], [1, 6]]\n",
    "            min_support:     int: The total number of occurences needed for an itemset to be considered frequent\n",
    "        \n",
    "        Outputs:\n",
    "            itemsets:        Dictionary of with keys as frequent itemset, and value as the total count of this itemset \n",
    "    \"\"\"\n",
    "    itemsets = dict()\n",
    "    ### TODO Your code here\n",
    "    \n",
    "    ### TODO Your code here\n",
    "    return itemsets\n",
    "\n",
    "def compute_candidates(prev_itemset):\n",
    "    Ck = set()\n",
    "    # Join step\n",
    "    for itemset in prev_itemset:\n",
    "        its1 = tuple(sorted(itemset))\n",
    "        for itemset2 in prev_itemset:\n",
    "            its2 = tuple(sorted(itemset2))\n",
    "            if its1[:-1] == its2[:-1]:\n",
    "                if its1[-1] < its2[-1]: Ck.add(its1 + its2[-1:])\n",
    "\n",
    "    # Pruning step\n",
    "    to_remove = set()\n",
    "    for c in Ck:\n",
    "        for subset in combinations(c, len(c)-1):\n",
    "            if not subset in prev_itemset:\n",
    "                to_remove.add(c)\n",
    "                break\n",
    "    for c in to_remove:\n",
    "        Ck.remove(c)\n",
    "    \n",
    "    return Ck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43863d2b-0c64-477a-9c56-70db4563bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR TEST CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-essence",
   "metadata": {},
   "source": [
    "### Task 2.2.2 (2 Points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> Run Apriori on the data-set (using the **kosarak** variable and **not kosarak_small**). Try a few different values of min_support. </br>\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Roughly how large does `min_support` need to be before no itemsets of size 2 are found? (Note that you don't need to find the exact value; reporting `min_support` to the nearest 1000 will suffice).\n",
    "\n",
    "Note that the dataset is reasonably large, so this **can take up a large amount of time depending on your value of min support and implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d4e28-a3e1-445b-936c-40204b2096a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "apriori_algorithm(kosarak, 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-department",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-copying",
   "metadata": {},
   "source": [
    "## Task 2.3 FP-Growth (9 Points)\n",
    "\n",
    "### Task 2.3.1 (7 Points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> Complete the implementation of FP-Growth below. You only need to implement growing the tree and building the header table. It is clearly marked where you need to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FP_Tree:\n",
    "    def __init__(self, T, min_support=10):\n",
    "        \"\"\"\n",
    "        Constructor for FP_Tree. Should correctly build an FP-Tree with header table.\n",
    "        Hint: I strongly advise you to implement the missing sections of the Node class before this one\n",
    "        \n",
    "        Inputs:\n",
    "            T:               A list of lists, each inner list will contiain integer-item-ids. \n",
    "                             Example: T = [[1, 2, 5], [2, 3, 4], [1, 6]]\n",
    "            min_support:     The total number of occurences needed to keep the itemset.\n",
    "        \"\"\"\n",
    "        self.min_support    = min_support\n",
    "        self.header_table   = {}\n",
    "        self.root           = Node(header_table = self.header_table)\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### Common functions for FP-tree and Conditional FP-tree\n",
    "    ### You do not need to modify the rest of this class\n",
    "    def generate_pattern(self, keys, support):\n",
    "        return tuple(keys + self.get_suffix()), support\n",
    "    \n",
    "    def get_suffix(self):\n",
    "        return []\n",
    "    \n",
    "    # This is the main function for generating frequent itemsets. You do not need to modify this,\n",
    "    # but I recommend reading and trying to understand it.\n",
    "    def mine_frequent_itemsets(self, res=None):\n",
    "        if res is None: res = []\n",
    "        \n",
    "        if self.root.is_single_path():\n",
    "            keys = list(self.header_table.keys())\n",
    "            key_idx = {k:i for i, k in enumerate(keys)}\n",
    "            counts = [self.header_table[k].count for k in keys]\n",
    "            \n",
    "            for key_pair in itertools.chain(*[itertools.combinations(keys, k) for k in range(1, len(keys)+1)]):\n",
    "                support = min([counts[key_idx[k]] for k in key_pair])\n",
    "                if support >= self.min_support: \n",
    "                    res.append(self.generate_pattern(list(key_pair), support))\n",
    "         \n",
    "        else: # Not single path\n",
    "            for key, node in self.header_table.items():\n",
    "                support = node.support()\n",
    "                \n",
    "                if support >= self.min_support:\n",
    "                    res.append( self.generate_pattern([key], support) )\n",
    "                \n",
    "                basis = []\n",
    "                while node is not None:\n",
    "                    curr_node = node\n",
    "                    node = node.nodelink\n",
    "                    \n",
    "                    if curr_node.parent is None:  continue\n",
    "                        \n",
    "                    path = curr_node.path(limit=curr_node.count)[:-1]\n",
    "                    if len(path) == 0:  continue\n",
    "                        \n",
    "                    basis.append( path )\n",
    "                    \n",
    "                if len(basis) == 0: continue\n",
    "                    \n",
    "                conditional_tree = Conditional_FP_Tree(self.min_support, [key] + self.get_suffix(), basis)\n",
    "                if conditional_tree.root is None: continue\n",
    "                    \n",
    "                conditional_tree.mine_frequent_itemsets(res=res)\n",
    "        return res\n",
    "\n",
    "\n",
    "# You don't need to modify anything in this class\n",
    "class Conditional_FP_Tree(FP_Tree):\n",
    "    def __init__(self, min_support, suffix, basis): \n",
    "        self.min_support    = min_support\n",
    "        self.suffix         = suffix\n",
    "        self.header_table   = {} # This will hold all unique items\n",
    "        \n",
    "        self.root           = Node(header_table=self.header_table)\n",
    "        \n",
    "        self.build_tree(basis)\n",
    "        # self.root           = prune(self.root, min_support)\n",
    "        if self.root is None: print(\"WARNING: root is empty after pruning\")\n",
    "        \n",
    "    def build_tree(self, basis):\n",
    "        for b in basis:\n",
    "            count = b[0][1]\n",
    "            path = list(map(lambda x: x[0], b))\n",
    "            for i in range(count):\n",
    "                self.root.add_path(path)\n",
    "    \n",
    "    def get_suffix(self):\n",
    "        return self.suffix\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, header_table, value=None, parent=None, path=None):\n",
    "        \"\"\"\n",
    "        Constructor for Node class, which is used for the FP-Tree. \n",
    "        Inputs:\n",
    "            header_table:    Dict. Should be same dict for all nodes in the tree\n",
    "            value:           Integer id of the item the node represents\n",
    "            parent:          Parent Node. None if root node\n",
    "            path:            List of node values for a path that should start in this node.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.children     = {}\n",
    "        self.header_table = header_table \n",
    "        self.nodelink     = None\n",
    "        self.value        = None\n",
    "        self.parent       = None\n",
    "        self.count        = 0\n",
    "        \n",
    "        if value is not None: # Only root node should have None as value\n",
    "            self.value          = value\n",
    "            self.parent         = parent\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "        \n",
    "        if path is not None: \n",
    "            self.add_path(path)\n",
    "            \n",
    "    \n",
    "    def add_path(self, path):\n",
    "        \"\"\"\n",
    "        Function for adding a path to tree. \n",
    "        Should follow an existing path and increment count while such a path exists. \n",
    "        If no path exists (or only partial path exists), this function should create or complete such a path\n",
    "        Hint: Recursion might be helpful.\n",
    "        Inputs:\n",
    "            path:            A list node values. \n",
    "                             Example: path = [1, 2, 5]\n",
    "        \"\"\"\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    # Functions for frequent items-sets and rule mining below. You do not need to modify these\n",
    "    def is_single_path(self):\n",
    "        if   len(self.children) == 0: return True \n",
    "        elif len(self.children) >  1: return False\n",
    "        else:  # len == 1\n",
    "            key = next((k for k in self.children.keys()))\n",
    "            return self.children[key].is_single_path()\n",
    "    \n",
    "    def support(self, verbose=False):\n",
    "        if verbose: print(\"Counting support, this value is \", self.value, \" with count \", self.count, \" and parent \", self.parent.value)\n",
    "            \n",
    "        if self.nodelink is not None: return self.count + self.nodelink.support(verbose)\n",
    "        else:                         return self.count\n",
    "    \n",
    "    def path(self, limit=-1):\n",
    "        if self.value is None: \n",
    "            return []\n",
    "        else:                  \n",
    "            count = self.count if limit == -1 else min(self.count, limit)\n",
    "            return self.parent.path(limit=limit) + [(self.value, count)]\n",
    "    \n",
    "    def print(self, indent=\"\", spacing=\"----|-\"):\n",
    "        print(indent + str(self.value) + \":\" + str(self.count))\n",
    "        for v in self.children.values():\n",
    "            v.print(indent=indent + spacing)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR TEST CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-lawrence",
   "metadata": {},
   "source": [
    "### Task 2.3.2 (2 Points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> Run FP-Growth on the data-set (using the **kosarak** variable and **not kosarak_small**). Try a few different values of min_support. </br>\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Roughly how large does `min_support` need to be before all itemsets of size 1 and 2 are found but no itemsets of size 3? (Note that you don't need to find the exact value; reporting `min_support` to the nearest 1000 will suffice).\n",
    "\n",
    "Note that the dataset is reasonably large, so this **can take up a large amount of time depending on your value of min support and implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "fp_tree = FP_Tree(kosarak, 30000)\n",
    "freq1 = fp_tree.mine_frequent_itemsets()\n",
    "print(freq1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-watts",
   "metadata": {},
   "source": [
    "## Task 2.4 Comparing A priori and FP-Growth (3 Points)\n",
    "Run the given experiment and show to what extent FP-Growth has an advantage.   \n",
    "<span style='color: green'>**\\[Describe\\]**</span> Comment on the results. What do you see? What did you expect to see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for testing the runtime of your algorithms. \n",
    "# WARNING: This will take a reasonably long time to run.\n",
    "    \n",
    "def sample(n=200, alphabet_size=5):\n",
    "    candidates  = np.array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'])[:alphabet_size]\n",
    "    m = candidates.shape[0]\n",
    "    \n",
    "    T = []\n",
    "    for i in range(n):\n",
    "        size = int(np.random.rand() * (m)) + 1\n",
    "        T.append(list(candidates[np.random.permutation(m)[:size]]))\n",
    "        \n",
    "    return T\n",
    "\n",
    "def test():\n",
    "    # If you want to test it quickly, you can modify \"transaction_lengths\" and \"alphabet_sizes\" temporarily. \n",
    "    # This will give you errors in the plotting (next code cell) though.\n",
    "    # Make sure you use the original values for \"transaction_lengths\" and \"alphabet_sizes\" for your final version.\n",
    "    transaction_lengths = [2**i for i in range(4, 11)]\n",
    "    alphabet_sizes      = [3, 6, 9, 12] \n",
    "    \n",
    "    min_support = 10\n",
    "    repeats     = 10\n",
    "    \n",
    "    stop = False\n",
    "    results = np.zeros((len(transaction_lengths), len(alphabet_sizes), 2))\n",
    "    stderrs = np.zeros((len(transaction_lengths), len(alphabet_sizes), 2))\n",
    "    \n",
    "#     print(results.shape)\n",
    "    \n",
    "    for i, n in enumerate(transaction_lengths):\n",
    "        for j, a in enumerate(alphabet_sizes):\n",
    "            print(\" - - \" * 4, \"n=%d,a=%d\" % (n, a), \" - - \" * 4)\n",
    "            times = []\n",
    "            for _ in range(repeats):\n",
    "                T = sample(n, a)\n",
    "\n",
    "                t0 = time.time()\n",
    "                tree = FP_Tree(T, min_support=min_support)\n",
    "                frequent_itemsets = tree.mine_frequent_itemsets()\n",
    "                t1 = time.time() - t0\n",
    "\n",
    "                i1 = {tuple(sorted(list(k))): v for k, v in frequent_itemsets}\n",
    "\n",
    "                t0 = time.time()\n",
    "                itemsets = apriori_algorithm(T, min_support=min_support)\n",
    "                t2 = time.time() - t0\n",
    "\n",
    "                i2 = {}\n",
    "                for V in itemsets.values():\n",
    "                    for k, v in V.items():\n",
    "                        i2[tuple(sorted(list(k)))] = v\n",
    "\n",
    "                assert len(i1) == len(i2)\n",
    "                for k in i1.keys():\n",
    "                    assert i1[k] == i2[k]\n",
    "\n",
    "                times.append([t1, t2])\n",
    "\n",
    "            results[i, j] = np.mean(times, axis=0)\n",
    "            stderrs[i, j] = np.std(times, axis=0)\n",
    "            print(np.mean(times, axis=0), \"+-\", np.std(times, axis=0), \"\\n\")\n",
    "            \n",
    "    np.save('itemsets_runningtimes', results)  # Results are saved to avoid having to run it again if plot code needs changing\n",
    "    np.save('itemsets_stderr', stderrs)\n",
    "    \n",
    "    return results, stderrs\n",
    "        \n",
    "results, stderrs = test()     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = np.load('itemsets_runningtimes.npy')\n",
    "stderrs = np.load('itemsets_stderr.npy')\n",
    "\n",
    "# Plotting \n",
    "transaction_lengths = [2**i for i in range(4, 11)]\n",
    "alphabet_sizes      = [3, 6, 9, 12] \n",
    "\n",
    "n, a, _ = results.shape\n",
    "res_to_plot = np.transpose(results, (1, 0, 2))\n",
    "err_to_plot = np.transpose(stderrs, (1, 0, 2))\n",
    "\n",
    "fig, ax = plt.subplots(1, a, figsize=(4*a, 4))\n",
    "for i, (res, err) in enumerate(zip(res_to_plot, err_to_plot)):\n",
    "    ax[i].plot(transaction_lengths, res[:,0], label='FP-Tree', color='C1')\n",
    "    ax[i].fill_between(transaction_lengths, res[:,0] - err[:,0], res[:,0] + err[:,0], alpha=0.3, linewidth=0 , color='C1')\n",
    "    \n",
    "    x = transaction_lengths[-1]\n",
    "    ax[i].set_xlim((2**4, 2**11))\n",
    "    ax[i].annotate(text='', xy=(x, res[-1,0]), xytext=(x,res[-1,1]), arrowprops=dict(arrowstyle='|-|'))\n",
    "    ax[i].annotate(text='%.1f $\\\\times$'%(res[-1,1]/res[-1,0]), xy=(x-24,  (res[-1,1] / 2 + res[-1,0]/2)), horizontalalignment='right')\n",
    "    \n",
    "    ax[i].plot(transaction_lengths, res[:,1], label='Apriori', color='C2')\n",
    "    ax[i].fill_between(transaction_lengths, res[:,1] - err[:,1], res[:,1] + err[:,1], alpha=0.3, linewidth=0 , color='C2')\n",
    "    \n",
    "    ax[i].set_title(\"Alphabet size: %d\" % alphabet_sizes[i])\n",
    "    ax[i].set_xscale('log', base=2)\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('Number of transactions')\n",
    "    ax[i].set_ylabel('Seconds')\n",
    "\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-vehicle",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8498360",
   "metadata": {},
   "source": [
    "# Part 3: Text Mining (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660b304",
   "metadata": {},
   "source": [
    "In this section we will work with textual data, more concretely with a subset of the [Wikipedia Dataset](https://huggingface.co/datasets/wikimedia/wikipedia).\n",
    "\n",
    "\n",
    "Run the code below to download and load the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rudygawron/wikipedia-20220301.simple-processed\", data_files=\"original_text.csv\", split=\"train\")\n",
    "dataset = dataset.to_pandas()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9109b0a",
   "metadata": {},
   "source": [
    "In the following exercises, we will try to embed the text from the dataset and do some document retrieval. Before we can do that, we need to clean the data. As you probably know from the lectures, a typical cleaning procedure involves:\n",
    "- converting text to UTF-8, e.g, “Cafée” → “Cafee”\n",
    "- lowercasing\n",
    "- Stemming or Lemmatization, e.g., “walking”, “walks”, “walked” → “walk”\n",
    "- stop-word removal e.g., “a”, “the”, “is”, “you”, “I”, punctuation marks\n",
    "\n",
    "All of that can be done with the function below.\n",
    "\n",
    "***[Hint]** It will probably be useful in the following exercises. So don't forget about it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model_name = \"en_core_web_sm\"\n",
    "if not importlib.util.find_spec(spacy_model_name):\n",
    "    spacy.cli.download(spacy_model_name)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809616e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str):\n",
    "    text = unicodedata.normalize('NFKD', text).encode(\"ascii\", \"ignore\").decode(\"utf-8\").lower()\n",
    "    doc = nlp(text)\n",
    "    return \" \".join(token.lemma_ for token in doc if token.is_alpha and not token.is_stop)\n",
    "\n",
    "clean(\"Héllo, thé quíck brówn föx jumpéd över thé lazy dög at 21:37!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d01790-7737-4004-bda4-a8818635f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR TEST CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfc6ec8",
   "metadata": {},
   "source": [
    "If you’ve tried cleaning the text in the dataset we use, you’ll quickly learn that it takes a lot of time to run. We’ve done this part for you.   \n",
    "Run the code below to download and load the cleaned Wikipedia dataset. Each row represents a cleaned sentence from the original dataset. The sentences can be grouped into paragraphs (by `paragraph_id`) or articles (by `article_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f54b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed Wikipedia dataset from Hugging Face\n",
    "processed_dataset = load_dataset(\"rudygawron/wikipedia-20220301.simple-processed\", data_files=\"processed_text.csv\", split=\"train\")\n",
    "wiki_df = processed_dataset.to_pandas()\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21c0c1",
   "metadata": {},
   "source": [
    "## Task 3.1 Similar document retrieval (warmup) (5 points)\n",
    "\n",
    "The goal is as follows. First, we want to convert every article in the dataset into an embedding. Then, given as input a document from the same dataset, we want to be able to retrieve the top-$k$ most similar documents."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d137301d",
   "metadata": {},
   "source": [
    "### Task 3.1.1 (1 points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> Using Singular Value Decomposition (SVD) compute embeddings of every document in the dataset. <br>\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "***[Hint]** First you need to merge `processed_text` into articles, then compute BOW matrix, convert it into TF-IDF matrix and after that you compute embeddings using SVD.*<br>\n",
    "<span style=\"color:red\">**[Note]** In your implementation you should use the imports in the cell below.</span> <br>\n",
    "<span style=\"color:red\">**[Note]** When using `TruncatedSVD` set `random_state=42` to ensure reproducibility.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "### TODO YOUR CODE BELOW\n",
    "\n",
    "# a numpy matrix of shape (#docs x d) where d is a hyperparameter that you have to choose yourself\n",
    "# i-th row should correspond to i-th row in `dataset`\n",
    "document_embeddings = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ec615",
   "metadata": {},
   "source": [
    "### Task 3.1.2 (3 points)\n",
    "<span style='color: green'>**\\[Describe\\]**</span> Why do we need to clean the text before embedding it? Take into consideration the following concepts from the lecture:\n",
    "- converting text to UTF-8\n",
    "- lowercasing\n",
    "- Lemmatization/Stemming\n",
    "- stop-word removal<br>\n",
    "- ...\n",
    "\n",
    "Give at least three arguments to justify your response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628e965",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span> <br>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ceacf0",
   "metadata": {},
   "source": [
    "### Task 3.1.3 (1 point)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the similar document retrieval function. Given the embedding of a document from the Wikipedia dataset and a parameter $k$, the function should return indices of the $k$ semantically closest articles. You can use either the cosine distance or the L2 distance as a measure of similarity.\n",
    "\n",
    "You can implement distance calculations yourself, but you have already done it in the previous assignments; that's why, here you can use the FAISS (Facebook AI Similarity Search) library. It's commonly used for efficient similarity search. <br>\n",
    "It contains two classes that we are interested in:\n",
    "- `faiss.IndexFlatL2` - for L2 similarity comparison\n",
    "- `faiss.IndexFlatIP` - for cosine similarity comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da860b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_docs(query: np.ndarray, k: int, index=None):\n",
    "    \"\"\"\n",
    "    Search for the top `k` most similar documents to the given query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : np.ndarray\n",
    "        The embedding of the query document.\n",
    "    k : int\n",
    "        The number of similar documents to retrieve.\n",
    "    index: \n",
    "        The FAISS index containing the document embeddings or just a numpy array with embeddings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of indices of the `k` closest embedded articles.\n",
    "    \"\"\"\n",
    "    ### TODO YOUR CODE THERE\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba347e",
   "metadata": {},
   "source": [
    "Run the code below to test your implementation.\n",
    "\n",
    "***[Hint]** If the most similar element to the query isn't the query itself, you've probably made a mistake somewhere.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc_index = 1194 # you can try different documents to get a feeling of how it works\n",
    "result_ids = search_similar_docs(document_embeddings[query_doc_index] , k=10)\n",
    "\n",
    "similar_titles = dataset.iloc[result_ids]['title'].tolist()\n",
    "similar_urls = dataset.iloc[result_ids]['url'].tolist()\n",
    "\n",
    "print(\"Query Document Title:\", dataset.iloc[query_doc_index]['title'])\n",
    "print(\"Similar Document Titles and URLs:\")\n",
    "for title, url in zip(similar_titles, similar_urls):\n",
    "    print(f\"- {title}: {url}\")"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "62487e56",
   "metadata": {},
   "source": [
    "## Task 3.2 Similar document retrieval (the actual exercise) (6 points)\n",
    "\n",
    "Now you will try a different approach for creating document embeddings. Instead of embedding documents directly using SVD, you will first embed words using SVD. <br>\n",
    "![image.png](attachment:image.png) <br>\n",
    "And then create document embeddings by taking average (or sum) of the embeddings of words present in the document. <br>\n",
    "![image-2.png](attachment:image-2.png) <br>\n",
    "An advantage of this approach is that, after learning word embeddings on the Wikipedia dataset, you can then create embeddings for any document, even those outside the dataset, and use them for similar document retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1821655",
   "metadata": {},
   "source": [
    "### Task 3.2.1 Implementation (6 points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the functions in the class below.\n",
    "\n",
    "A) `train` - As input, takes an array of `texts` and a parameter `d`. First, you need to convert `texts` to a TF-IDF matrix and then create `d`-dimensional word embeddings using SVD. Store the embeddings somehow, you will need them to implement `store` and `retrieve`. You will also need to know which embedding corresponds to which word. If you are using `CountVectorizer()` to create the BOW matrix, you can get the list of words with `get_feature_names_out()`. <br>\n",
    "\n",
    "B) `store` - As input, takes an array of `docs`. Converts them into `d`-dimensional document embeddings by taking the average of the embeddings of words present in the document. Again, you need to store the embeddings somehow, you will need them in `retrieve`. <br>\n",
    "\n",
    "C) `retrieve` - As input, takes an array of text `queries` and a parameter `k`. Converts the `queries` into document embeddings. For each `query` in `queries`, returns indices of `k` closest `docs` stored in `store` function. Use **L2 distance** there. Returned `docs` should be ordered from closest to furthest. The returned matrix should have shape (#queries x k).<br>\n",
    "\n",
    "You can assume that text inputs are already cleaned.\n",
    "\n",
    "<span style=\"color:red\">**[Note]** When using `TruncatedSVD` set `random_state=42`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3927c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class DocumentRetrieval():\n",
    "\n",
    "    def train(self, texts, d: int) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def store(self, docs) -> None:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def retrieve(self, queries, k: int) -> np.ndarray:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093efa92",
   "metadata": {},
   "source": [
    "Now, you will test the performance of your similar document retrieval algorithm on a benchmark. Run the code below to download a dataset you will test your algorithm against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "\n",
    "dataset = \"scifact\"\n",
    "data_path = util.download_and_unzip(f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\", \"./data\")\n",
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddf3c5",
   "metadata": {},
   "source": [
    "`corpus` contains docs that you want to store using `store` and later retrieve using `retrieve`. <br>\n",
    "`queries` contains queries that you want to pass as argument to `retrieve`. <br>\n",
    "`qrels` contains ground truth retrieval results used in the benchmark.\n",
    "\n",
    "Run the code below to clean `corpus` and `queries`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9187d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df = pd.DataFrame.from_dict(corpus, orient='index').reset_index().rename(columns={'index': 'corpus_id'})\n",
    "corpus_df.reset_index(drop=True, inplace=True)\n",
    "corpus_df.drop(columns=['title'], inplace=True)\n",
    "corpus_df['processed_text'] = [clean(text) for text in tqdm(corpus_df['text'])]\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pd.DataFrame(list(queries.items()), columns=['query_id', 'query_text'])\n",
    "queries_df['processed_text'] = [clean(doc) for doc in tqdm(queries.values())]\n",
    "queries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb35409",
   "metadata": {},
   "source": [
    "Run the test below that check the accuracy of you algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = DocumentRetrieval()\n",
    "\n",
    "# For the training we pass all sentences from Wikipedia dataset\n",
    "dr.train(wiki_df['processed_text'], d=50)\n",
    "\n",
    "# store documents from the corpus\n",
    "dr.store(corpus_df['processed_text'])\n",
    "\n",
    "# return most similar docs\n",
    "I = dr.retrieve(queries_df['processed_text'], k=100)\n",
    "\n",
    "def test(I):\n",
    "    I_dict = {query_id: {corpus_df.iloc[id]['corpus_id']: -i for i, id in enumerate(I_row)} for query_id, I_row in zip(queries_df['query_id'], I)}\n",
    "    return top_k_accuracy(qrels, I_dict, [1, 3, 5, 10, 100])\n",
    "\n",
    "test(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382badd2",
   "metadata": {},
   "source": [
    "## Task 3.3 Upgrading the model (9 points)\n",
    "\n",
    "Your task is to **introduce, test, and reason** about the result of the following modifications to the algorithm. If some modification results in a better model performance, keep it, so that at the end of the exercise, you end up with the best-performing model. <br>\n",
    "\n",
    "<span style=\"color:red\">**[Note]** When performing experiments, take into consideration how you present the results, i.e., the aesthetics.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57371b",
   "metadata": {},
   "source": [
    "### Task 3.3.1 (3 points)\n",
    "\n",
    "**A).** <span style='color: green'>**\\[Implement\\]**</span> Currently in `train`, you use `TfidfTransformer` in order to create a TF-IDF matrix and then perform SVD on it to get word embeddings. Now, instead of using the TF-IDF matrix, you will use the pointwise mutual information (PMI) matrix. Unfortunately, `sklearn` doesn't provide something like `PMITransformer`. But that's not a problem, you are computer scientists, you can implement it yourselves.<br>\n",
    "\n",
    "**Steps to Implement a PMI Transformer:**\n",
    "1. In the code below, you can assume that `X` is an output from `CountVectorizer`, which means it is a co-occurrence matrix where rows represent sentences and columns represent words.\n",
    "2. Estimate probabilities:\n",
    "   - $ P(c, w) = \\frac{X[c, w]}{\\sum_{i,j} X[i,j]} $\n",
    "   - $ P(c) = \\frac{\\sum_j X[c, j]}{\\sum_{i,j} X[i,j]} $\n",
    "   - $ P(w) = \\frac{\\sum_i X[i, w]}{\\sum_{i,j} X[i,j]} $\n",
    "3. Compute PMI:\n",
    "   $\\text{PMI}[c, w] = \\log \\left( \\frac{P(c, w)}{P(c)P(w)} \\right)$\n",
    "4. Clip PMI values to 0 to get Positive PMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c27e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "class PMITransformer:\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the PMITransformer to the given co-occurrence matrix.\n",
    "        \"\"\"\n",
    "        if not sparse.issparse(X):\n",
    "            X = sparse.csr_matrix(X)\n",
    "\n",
    "        ### TODO YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "        ### TODO YOUR CODE ENDS HERE\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the given co-occurrence matrix into a PMI matrix.\n",
    "        \"\"\"\n",
    "        if not sparse.issparse(X):\n",
    "            X = sparse.csr_matrix(X)\n",
    "\n",
    "        X_coo = X.tocoo()\n",
    "        rows, cols = X_coo.row, X_coo.col\n",
    "        \n",
    "        ### TODO YOUR CODE STARTS HERE\n",
    "        pmi_data = ...\n",
    "        \n",
    "        ### TODO YOUR CODE ENDS HERE\n",
    "        return sparse.coo_matrix((pmi_data, (rows, cols)), shape=X.shape)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f10bf",
   "metadata": {},
   "source": [
    "**B).** <span style='color: green'>**\\[Implement & Test\\]**</span> Plug `PMITransformer` (which you implemented above) into `DocumentRetrieval` and test the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64983ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d656975f",
   "metadata": {},
   "source": [
    "**C).** <span style='color: green'>**\\[Motivate\\]**</span> What do you observe? Justify your answer by reasoning about the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dafa73",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span> <br>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080d48f",
   "metadata": {},
   "source": [
    "### Task 3.3.2 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b10d123",
   "metadata": {},
   "source": [
    "**A).** <span style='color: green'>**\\[Implement & Test\\]**</span> Currently, you `train` your model by performing SVD on a matrix where each row represents a single sentence. Try: <br>\n",
    "&emsp;1). grouping sentences into paragraphs (by `paragraph_id`) <br>\n",
    "&emsp;2). grouping sentences into articles (by `article_id`) <br>\n",
    "&emsp;3). performing a sliding window on each sentence. Use `step size` = 3 and `window size` = 7.<br>\n",
    "\n",
    "***[Hint]** Don't know what a sliding window is? Consider the following example.   \n",
    "Let the input sentence be: \"Africa is the world's second-largest and second-most populous continent after Asia\".   \n",
    "After cleaning, it becomes: \"africa world second large second populous continent asia\".   \n",
    "After applying a sliding window with `step size` = 2 and `window size` = 3, it becomes: [\"africa world second\", \"second large second\", \"second populous continent\", \"continent asia\"].*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4ac73c",
   "metadata": {},
   "source": [
    "B). <span style='color: green'>**\\[Describe\\]**</span> What do you observe? Is this something that you expected? Justify your answer by reasoning about the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b7a3b1",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span> <br>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d2b675",
   "metadata": {},
   "source": [
    "### Task 3.3.3 (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aab7a9",
   "metadata": {},
   "source": [
    "**A).** <span style='color: green'>**\\[Implement & Test\\]**</span> Currently you used L2 distance in order to compute document similarities. Try cosine similarity instead. Then run the following experiment. For both L2 and cosine distance, test the performance for different values of word embedding dimensionality `d`. Try `d = 25, 50, 75, 100, 200, 300, 400, and 500` (but it's encouraged to try more if you think it brings some value to the experiment). Plot the results of the top-100 accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e58a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This will take a reasonably long time to run.\n",
    "### TODO YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d84546",
   "metadata": {},
   "source": [
    "B). <span style='color: green'>**\\[Describe\\]**</span> What do you observe? Is this something that you expected? If yes, why? If no, why not?\n",
    "\n",
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span> <br>\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba25ac",
   "metadata": {},
   "source": [
    "### Task 3.3.4 (0 points)\n",
    "\n",
    "Try thinking of other ways to improve the model, implement, and test those ideas. You don't get points for this exercise, but by doing it, you might learn something new. We will, of course, give you feedback for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07515724-665c-42bf-bd6b-914958f26f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
