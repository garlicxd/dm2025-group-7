{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "overall-spider",
   "metadata": {},
   "source": [
    "# Data Mining - Handin 1 - Clustering \n",
    "Welcome to the handin on clustering algorithms and outlier detection. \n",
    "This handin corresponds to the topics in Week 5--9 in the course.\n",
    "\n",
    "The handin is \n",
    "* done in the chosen handin groups\n",
    "* worth 10% of the final grade\n",
    "\n",
    "For the handin, you will prepare a report in PDF format, by exporting the Jupyter notebook. \n",
    "Please submit\n",
    "1. The jupyter notebook file with your answers\n",
    "2. The PDF obtained by exporting the jupyter notebook\n",
    "\n",
    "**The grading system**: Tasks are assigned a number of points based on the difficulty and time to solve it. The sum of the number of points is 100. For the maximum grade you need to get at least _90 points_. The minimum grade (02 in the Danish scale)\n",
    "requires **at least** 30 points, with at least 8 points from the first three Parts (Part 1,2,3) and 6 points in the last part (Part 4).\n",
    "\n",
    "**The exercise types**: There are four different types of exercises\n",
    "1. <span style='color: green'>**\\[Compute by hand\\]**</span> means that you should provide NO code, but show the main steps to reach the result (not all). \n",
    "2. <span style='color: green'>**\\[Motivate\\]**</span> means to provide a short answer of 1-5 lines indicating the main reasoning, e.g., the PageRank of a complete graph is 1/n in all nodes as all nodes are symmetric and are connected one another.\n",
    "3. <span style='color: green'>**\\[Prove\\]**</span> means to provide a formal argument and NO code. \n",
    "4. <span style='color: green'>**\\[Implement\\]**</span> means to provide an implementation. Unless otherwise specified, you are allowed to use helper functions (e.g., ```np.mean```, ```itertools.combinations```, and so on). **However**, if the task is to implement an algorithm, by no means a call to a library that implements the same algorithm will be deemed as sufficient!\n",
    "\n",
    "**Q&A**\n",
    "\n",
    "Q: If the task is to implement a mean function, may I just call ```np.mean()```? \n",
    "<br>A: No.\n",
    "\n",
    "Q: If the task is to compare the mean of X and Y, may I use ```np.mean()``` to calculate the mean?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: If I have implemented a mean function in a previous task, but I am unsure of its correctness, may I use ```np.mean()``` in following task where mean is used as a helper function? \n",
    "<br>A: Yes.\n",
    "\n",
    "Q: May I use ```np.mean()``` to debug my implementation of mean?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Do I get 0 points for a task if I skip it?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Can I get partial points for a task I did partially correct?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Is it OK to skip a task if I do not need the points from it?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Should I inform a TA if I find an error?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Should I ask questions if I am confused?\n",
    "<br>A: Yes.\n",
    "\n",
    "\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "loaded-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT TOUCH\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "RANDOM_SEED = 132414\n",
    "## DO NOT TOUCH\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target\n",
    "color_map = {0:'Blue', 1:'Red', 2:'Green'}\n",
    "toy = df.sample(n=15, random_state=RANDOM_SEED)\n",
    "print(\"Is working\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-regression",
   "metadata": {},
   "source": [
    "# Part 1 Intro Excercises\n",
    "\n",
    "## Task 1.1 K-Means and DBScan\n",
    "\n",
    "### Task 1.1.1 (4 points)\n",
    "<span style='color: green'>**\\[Compute by hand\\]**</span> the cluster assignments _for the dataset below_ using k-means and $k = 2$, with initial centroids being (2.2, 0.9) and (2.3,  1.0)\n",
    "\n",
    "<font color='red'>To evaluate (i.e., only to control the correctness and not to solve the exercise) your results you can use **sklearn.cluster.KMeans**.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-matter",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "annoying-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = 'ash'\n",
    "last = 'hue'\n",
    "\n",
    "X_kmeans = toy[[first, last]]\n",
    "\n",
    "plt.scatter(X_kmeans[first], X_kmeans[last], alpha=0.8, c=toy['target'].map(color_map))\n",
    "plt.axis('equal')\n",
    "\n",
    "coordinates = X_kmeans.values\n",
    "targets = toy['target'].values\n",
    "\n",
    "\n",
    "# Extra code => Check points\n",
    "# for i, (x,y) in enumerate(coordinates):\n",
    "#     print(f\"Point {i+1}: ({x}, {y}) - Target: {targets[i]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc382d-43c2-471b-ad9d-a946f663a3cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## K-Means Clustering (Lloyd's Algorithm) with $k=2$\n",
    "\n",
    "### Introduction\n",
    "We begin with **two initial centroids**:\n",
    "$$\n",
    "\\text{centroid}_1 = (2.2, 0.9), \\quad \\text{centroid}_2 = (2.3, 1.0)\n",
    "$$\n",
    "\n",
    "Since we do not have predefined cluster assignments, we will **assign each point to the closest centroid** by computing the squared Euclidean distance between each point and the centroids.\n",
    "\n",
    "### Cluster Assignment\n",
    "For **Point 1**, the squared Euclidean distance to each centroid is computed as:\n",
    "\n",
    "$$\n",
    "\\text{dist}(\\text{point}_1, \\text{centroid}_1) = \\sum_{i=1}^{d} ( \\text{point}_{1i} - \\text{centroid}_{1i})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (2.19 - 2.2)^2 + (1.06 - 0.9)^2 = 0.0257\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{dist}(\\text{point}_1, \\text{centroid}_2) = (2.19 - 2.3)^2 + (1.06 - 1)^2 = 0.0157\n",
    "$$\n",
    "\n",
    "Since **Point 1** is **closer to centroid 2**, it is assigned to **Cluster 2**.  \n",
    "Repeating this for all of the remaining **14 points**, we obtain:\n",
    "\n",
    "$$\n",
    "\\text{Cluster}_1 = \\{ \\text{Point}_5, \\text{Point}_6, \\text{Point}_9, \\text{Point}_{10}, \\text{Point}_{12}, \\text{Point}_{13}, \\text{Point}_{14}, \\text{Point}_{15} \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Cluster}_2 = \\{ \\text{Point}_1, \\text{Point}_2, \\text{Point}_3, \\text{Point}_4, \\text{Point}_7, \\text{Point}_8, \\text{Point}_{11} \\}\n",
    "$$\n",
    "\n",
    "Given that:\n",
    "\n",
    "$$\n",
    "\\text{centroid}_1 \\in \\text{Cluster}_1, \\quad \\text{centroid}_2 \\in \\text{Cluster}_2\n",
    "$$\n",
    "\n",
    "### Updating the Centroids\n",
    "We **recalculate the centroids** by taking the **mean of all points in each cluster**.\n",
    "\n",
    "#### **New Centroid 1 Calculation**:\n",
    "$$\n",
    "\\text{new centroid}_1 = \\frac{1}{|\\text{Cluster}_1|} \\sum_{\\text{Point}_i \\in \\text{Cluster}_1} \\text{Point}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1}{8} \\left( \n",
    "\\begin{bmatrix} 1.82 \\\\ 0.75 \\end{bmatrix} + \n",
    "\\begin{bmatrix} 2.38 \\\\ 0.79 \\end{bmatrix} + \n",
    "\\begin{bmatrix} 2.14 \\\\ 1.05 \\end{bmatrix} + \n",
    "\\begin{bmatrix} 1.99 \\\\ 0.95 \\end{bmatrix} + \n",
    "\\begin{bmatrix} 2.00 \\\\ 0.93 \\end{bmatrix} + \n",
    "\\begin{bmatrix} 2.17 \\\\ 0.86 \\end{bmatrix} + \n",
    "\\begin{bmatrix} 2.10 \\\\ 0.58 \\end{bmatrix} + \n",
    "\\begin{bmatrix} 1.98 \\\\ 0.7 \\end{bmatrix} \n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\begin{bmatrix} 2.0725 \\\\ 0.82625 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **New Centroid 2 Calculation**:\n",
    "$$\n",
    "\\text{new centroid}_2 = \\frac{1}{|\\text{Cluster}_2|} \\sum_{\\text{Point}_i \\in \\text{Cluster}_2} \\text{Point}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\begin{bmatrix} 2.108 \\\\ 0.916 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **Convergence & Final Clusters**\n",
    "We repeat the **cluster assignment and centroid update** process until the **clusters remain unchanged**, meaning the algorithm **converges**.\n",
    "\n",
    "After convergence, the **final clusters** are:\n",
    "\n",
    "$$\n",
    "\\text{Cluster}_1 = \\{ \\text{Point}_5, \\text{Point}_9, \\text{Point}_{10}, \\text{Point}_{12}, \\text{Point}_{13}, \\text{Point}_{14}, \\text{Point}_{15} \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Cluster}_2 = \\{ \\text{Point}_1, \\text{Point}_2, \\text{Point}_3, \\text{Point}_4, \\text{Point}_6, \\text{Point}_7, \\text{Point}_8, \\text{Point}_{11} \\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-somerset",
   "metadata": {},
   "source": [
    "### Task 1.1.2 (3 points)\n",
    "<span style='color: green'>**\\[Compute by hand\\]**</span> <br>\n",
    "A) Using examples, show why the k-means algorithm may not find the global optimum. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-vertex",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "Global minimum means that we find the optimized clustering of the data points such that the accumulated distance between each point and their cluster representatitve centroid is the smallest. Unfortunately, this is not guaranteed using K-means, due to its random initialization.\n",
    "  An example will be to have four points, let's call them Point1, Point2, Point3 and Point4. \n",
    "\n",
    "Between Point1 and Point2, as well as between Point3 and Point4, the distance is of epsilon (small), but the distance between Point1 and Point3, as well as between Point2 and Point4 is of a huge number N. By randomly selecting Point1 and Point2 being the centroids, we will get the clusters (Point1, Point3) in Cluster1 and (Point2, Point4) in Cluster2. The final updated centroid for Cluster1 will be (Point1 + Point3) / 2  and for Cluster2 is (Point2+Point4) / 2, more concretely, approximately at N/2 distance, where N huge number.\n",
    " The optimal solution would have been to look globally at the relation between the points, seeing that Point1 and Point2 are extremely close, as well as Point3 and Point4, therefore better to have selected centroids who would have created the clusters Cluster1 (Point1, Point2) and Cluster (Point3, Point4), having centroids at distance epsilon/2 distance, where epsilon small, therefore the overall cost of the sum of the distances between each point and its centroid will be drastically smaller regarding the optimal solution than the local random based on. \n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-chicago",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span> <br>\n",
    "B) K-means vs K-medoids: Which performs better when the dataset contains outliers, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-office",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "K-medoids performs better given outliers, because cluster representants are actual points and not means calculated by taking the means of the points from the specific cluster. Given the medoid algorithm, regarding  updating the medoids in order to improve by minimizing cost of the accumulated distance between medoids and points, this happens via swapping between medoid and non medoid point, given that the non medoid point minimizes more the cost than the medoid point. The outlier, because very far apart the cluster copmtared to it's cluster based points, will never be the medoid, therefore it's influence is minimal, in contrast to the k-means, were it influences by affecting the centroid point when taking the average of all the points, including the outlier. \n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-management",
   "metadata": {},
   "source": [
    "### Task 1.1.3 (4 points)\n",
    "<span style='color: green'>**\\[Compute by hand\\]**</span> the dendrogram for the dataset of Task 1.1.1. using **complete-link**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-template",
   "metadata": {},
   "source": [
    "*******************\n",
    "\n",
    "Dendogram explains the order of Hierarchical Clustering - using complete-link in this case - in a dataset.\n",
    "For it we need to follow a few steps:\n",
    "\n",
    "1. Initialization: Begin by considering each data point as a separate cluster. Write them up on in a dendogram, with each cluster as a node.\n",
    "\n",
    "2. Distance Matrix Calculation: Create a matrix where each entry (i, j) represents the distance between point i and point j, using the euclidean distance for this calculation. (Refer to Task 1.1.1 for an example.)\n",
    "    - The distance between two clusters $C_i$ and $C_j$ in complete-linkage clustering is defined as the maximum distance between any point $x$ in $C_i$ and any point $y$ in $C_j$: $dist_{cl}(C_i, C_j) = \\max_{x \\in C_i, y \\in C_j} dist(x, y)$.\n",
    "\n",
    "3. Complete-Link Clustering\n",
    "    - Find and merge the two closest clusters using the distance matrix.\n",
    "    - On the dendrogram, create a new level by merging the two nodes corresponding to the merged clusters.\n",
    "\n",
    "4. Update the Distance Matrix by calculating each cluster's distance to the new cluster.\n",
    "\n",
    "5. Jump to point **3.** until there's only one cluster.\n",
    "\n",
    "#### Example using the given dataset\n",
    "\n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-benjamin",
   "metadata": {},
   "source": [
    "### Task 1.1.4 (3 points)\n",
    "A) <span style='color: green'>**\\[Compute by hand\\]**</span> the density-based clustering DBSCAN for the dataset of Task 1.1.1 using $\\epsilon=0.2$ and $MinPts=3$. Present at least 2 iterations of the algorithm<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-emperor",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "\n",
    "**Related to Density Based Clustering => Week 3!!**\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-briefs",
   "metadata": {},
   "source": [
    "\n",
    "B) <span style='color: green'>**\\[Motivate\\]**</span> the difference between the clusters obtained with DBSCAN and those obtained with KMeans in Task 1.1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-programming",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "**Related to Density Based Clustering => Week 3!!**\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-fields",
   "metadata": {},
   "source": [
    "## Task 1.2 Elliptic data set (2 points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> <br> \n",
    "After looking at the dataset _below_, you want to detect the red outlier point, assuming you know that it is an outlier. \n",
    "\n",
    "Which approach would be the most obvious to find the red outlier? Please (1) check the box and (2) motivate your answer below:\n",
    "- [ ] Distance based approach (with parameteres $\\pi=0.5$, $\\epsilon=2$ and euclidean distance)\n",
    "- [ ] Angle based approach\n",
    "- [ ] Depth based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_new = np.array([[1.5, 1.0], # Red \n",
    "                [1., 1.0],\n",
    "                [0.5, 0.5],\n",
    "                [1, 0.5],\n",
    "                [0.5, 1],\n",
    "                [0.75, 0.75]\n",
    "                 ])\n",
    "\n",
    "plt.scatter(D_new[:, 0], D_new[:, 1], alpha=0.8, c = ['red' if i == 0 else 'blue' for i in range(len(D_new))])\n",
    "plt.axis([0, 2, 0, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-tuesday",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "\n",
    "\n",
    "**Related to Outlier Detection => Week 5!**\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-extraction",
   "metadata": {},
   "source": [
    "## Task 1.3 Theoretical questions\n",
    "### Task 1.3.1 Statistical Analysis (3 points)\n",
    "<span style='color: green'>**\\[Prove\\]**</span> 1. Prove that the Euclidean distance is a pseudometric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-column",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "Pseudometry is concerned about the generalization of a metric, a distance function, with the following conditions:\n",
    "\n",
    "- Non-negativity d(x,y) $\\geq$ 0\n",
    "- Symmetry: d(x,y) = d(y,x)\n",
    "- Triangle inequality d(x,z) $\\leq$ d(x,y)+ d(y,z)\n",
    "\n",
    "\n",
    "To prove that the Euclidean distance is pseudometric, we need to prove that the Euclidean distance is non-negative, symmetric and follows the triangle inequality. The Euclidean distance is derived from pythagoras theorem. Given 2D euclidean, where we have two points: P_1 = (x_1, y_1) and P_2 = (x_2, y_2), the euclidean distance is of:\n",
    "d(P_1, P_2) = $\\sqrt{(x_2 - x_1) + (y_2 - y_1)}$.  This means that we have non-negativity, because the square of all real numbers, no matter positive or negative, result in a positive number. We add those squares and then take the square root, therefore impossible to have a negative output, making  the euclidean distance respect the non-negativity condition\n",
    "\n",
    "Regarding Symmetry, d(P_1, P_2) = d(P_2, P_1) because we take the square of the difference between the points values of each dimensions. More general, assume analysing a particular dimension, having values a and b, we need to prove that  $(a-b)^2 = (b-a)^2$. This is simply proven by expanding both equations and ultimately getting that: $(a-b)^2 = (b-a)^2 = -2ab + a^2 + b^2$. \n",
    "\n",
    "Regarding Triangle Inequality, we start by proving that given points p and q,  $$\\Rightarrow |p+q| \\leq |p| + |q|$$ holds, using Cauchy Schwarz inequality \n",
    "\n",
    "$$\n",
    "|p+q|^2 = (p+q) \\cdot (p+q) = |p|^2 + 2(p \\cdot q) + |q|^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\leq |p|^2 + 2|p \\cdot q| + |q|^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\leq |p|^2 + 2|p||q| + |q|^2 = (|p| + |q|)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow |p+q| \\leq |p| + |q|\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Given that we have the three points  p, q, r, we have then, given using the conclusion from before, that:}\n",
    "$$\n",
    "\n",
    "$$\n",
    "d(p,r) = |p - r| = |p - q + q - r|\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\leq |p - q| + |q - r| = d(p,q) + d(q,r)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbd657",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Prove\\]**</span> 2. Prove that the sample mean is an unbiased estimator of the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c9cdc",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "\n",
    "We know that the sample mean is the following:\n",
    "\n",
    "$u^{hat} = \\frac{1}{n} \\cdot \\sum^{n}_{i=1}x_i$\n",
    "\n",
    "\n",
    "The population mean is the expected value, which can be written as:\n",
    "\n",
    "$u = E[X] = \\sum_{x} x \\cdot f(x)$\n",
    "\n",
    "\n",
    "To prove that $u^{hat}$ is the unbiased estimator, we should show that for parameter $\\theta $, $E[\\theta^{hat}] = \\theta$, for every possible $\\theta$\n",
    "\n",
    "\n",
    "So, let's begin:\n",
    "\n",
    "$E[u^{hat}] = E[ \\frac{1}{n} \\cdot \\sum^{n}_{i=1} x_{i}] = \\frac{1}{n} \\sum^{n}_{i=1}E[x_{i}] = \\frac{1}{n} \\sum^{n}_{i=1} u = u$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b431d",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Prove\\]**</span> 3. Prove that the sample variance is an asymptotically unbiased estimator of the population variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831962c",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "\n",
    "Given the formula for the variance, we have:\n",
    "\n",
    "\n",
    "$\\sigma^{2} = var(X) = E[(X-u)^{2}]$\n",
    "\n",
    "whereas sample variance is:\n",
    "\n",
    "$\\sigma_{squared}^{2} = \\frac{1}{n} \\sum^{n}_{i=1} (x_{i} = u^{hat})^{2}$\n",
    "\n",
    "\n",
    "Now, let's prove now that it is asymptotically unbiased, but first let' assume that it is actually a biased estimator, meaning that $E[\\sigma_{hat}^{2}] \\neq \\sigma^{2}$\n",
    "\n",
    "\n",
    "More concretely, from the book, we can conclude from equation 2.19, that:\n",
    "\n",
    "$E[\\sigma^{2}] = E[\\frac{1}{n} \\sum^{n}_{i=1} (x_{i}- u_{hat})^{2}] = E[\\frac{1}{n} \\sum^{n}_{i=1}(x_{i} - u)^{2}] - E[(u_{hat} - u)^{2}]$\n",
    "\n",
    "And combine with the book equation 2.17, saying\n",
    "\n",
    "$E[(u_{hat} - u)^{2}] = \\frac{\\sigma^{2}}{n} $\n",
    "\n",
    "We get:\n",
    "\n",
    "$E[\\sigma_{hat}^{2}] = \\frac{1}{n} n\\sigma^{2} - \\sigma^{2}/n = (\\frac{n-1}{n}) \\sigma^{2}$\n",
    "\n",
    "But this means that it's asymptotically biased, by taking the limit of n going to infinity, the bias is no longer valid, meaning that:\n",
    "\n",
    "$E[\\sigma_{hat}^{2}] \\rightarrow \\sigma^{2} $ given $n \\rightarrow \\infty$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bf551",
   "metadata": {},
   "source": [
    "### Task 1.3.2 Kernel trick (7 points)\n",
    "<span style='color: green'>**\\[Motivation\\]**</span> A) What is a positive-definite kernel $K(x,x')$ of two vectors $x,x'\\in\\mathbb{R}^n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0696e0",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "\n",
    "Regarding a kernel $K(x,x')$, we know that is should satisfy the condition:\n",
    "\n",
    "$ K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j) $ given $\\phi(\\mathbf{x})$ is a function from input space to feaature space. \n",
    "\n",
    "\n",
    "\n",
    "A positive definite means that we have a matrix $A \\in \\mathbb{R}^{n x n}$ , where for every nonzero vector $x \\in \\mathbb{R}^{n}$, we have:\n",
    "\n",
    "$x^{T}Ax > 0$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249d778",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span> B) Please explain briefly what is the kernel trick method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0ba08",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe260d",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Prove\\]**</span> C)\n",
    "Let two positive-definite kernels $K_1(x,x')$ and $K_2(x,x')$.  <br> Show that functions $K_1(x,x')+K_2(x,x')$ and $K_1(x,x')K_2(x,x')$ are also positive-definite kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b07b7",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94954d",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Prove\\]**</span> D) Prove that $K(x,x')=e^{2 ln(x^{\\top}x')-(x-x')^{\\top}(x-x')}$ is a positive-definite kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f812c",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-artwork",
   "metadata": {},
   "source": [
    "# Part 2 Exploratory data analysis\n",
    "In this section, you will perform preliminary analysis on your data. These preliminary analysis are useful to understand how the data behaves, before running complex algorithms.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "guilty-vegetable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  target  \n",
       "0                          3.92   1065.0       0  \n",
       "1                          3.40   1050.0       0  \n",
       "2                          3.17   1185.0       0  \n",
       "3                          3.45   1480.0       0  \n",
       "4                          2.93    735.0       0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy = df\n",
    "data_np = toy.to_numpy()\n",
    "headers = ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium','total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
    "       'proanthocyanins', 'color_intensity', 'hue','od280/od315_of_diluted_wines', 'proline', 'target']\n",
    "X = data_np[:,3:]\n",
    "y = data_np[:,11]\n",
    "rows, cols = np.shape(X)\n",
    "toy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-ottawa",
   "metadata": {},
   "source": [
    "## Task 2.1 Correlation matrix\n",
    "### Task 2.1.1 (5 points)\n",
    "A) <span style='color: green'>**\\[Implement\\]**</span> in the code-box below the **correlation matrix** (not covariance matrix) among all the attributes. <br>\n",
    "<font color='red'>To CHECK your results you can use **numpy.corrcoef**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "occasional-seeking",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m X \u001b[38;5;241m=\u001b[39m data_np\n\u001b[0;32m     11\u001b[0m Corr \u001b[38;5;241m=\u001b[39m correlation_matrix(X)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCorr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mcolorbar()\n",
      "File \u001b[1;32md:\\MiniConda\\envs\\dm25\\lib\\site-packages\\matplotlib\\pyplot.py:2248\u001b[0m, in \u001b[0;36mmatshow\u001b[1;34m(A, fignum, **kwargs)\u001b[0m\n\u001b[0;32m   2244\u001b[0m     ax \u001b[38;5;241m=\u001b[39m gca()\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2246\u001b[0m     \u001b[38;5;66;03m# Extract actual aspect ratio of array and make appropriately sized\u001b[39;00m\n\u001b[0;32m   2247\u001b[0m     \u001b[38;5;66;03m# figure.\u001b[39;00m\n\u001b[1;32m-> 2248\u001b[0m     fig \u001b[38;5;241m=\u001b[39m figure(fignum, figsize\u001b[38;5;241m=\u001b[39m\u001b[43mfigaspect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2249\u001b[0m     ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_axes([\u001b[38;5;241m0.15\u001b[39m, \u001b[38;5;241m0.09\u001b[39m, \u001b[38;5;241m0.775\u001b[39m, \u001b[38;5;241m0.775\u001b[39m])\n\u001b[0;32m   2250\u001b[0m im \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mmatshow(A, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\MiniConda\\envs\\dm25\\lib\\site-packages\\matplotlib\\figure.py:3609\u001b[0m, in \u001b[0;36mfigaspect\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m   3607\u001b[0m \u001b[38;5;66;03m# Extract the aspect ratio of the array\u001b[39;00m\n\u001b[0;32m   3608\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isarray:\n\u001b[1;32m-> 3609\u001b[0m     nr, nc \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   3610\u001b[0m     arr_ratio \u001b[38;5;241m=\u001b[39m nr \u001b[38;5;241m/\u001b[39m nc\n\u001b[0;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "def correlation_matrix(X):\n",
    "    corr = None\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE \n",
    "    return corr\n",
    "    \n",
    "X = data_np\n",
    "Corr = correlation_matrix(X)\n",
    "plt.matshow(Corr)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-volume",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "B) By observing the  **correlation matrix** in A), which pair(s) of different features has the highest correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-silver",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-senator",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "C) What does it mean that two features are highly correlated? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-breath",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-springfield",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "D) Based on the features of the data in Part 2 and your answer in C), did you expect the observation of B)? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-weekly",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-content",
   "metadata": {},
   "source": [
    "### Task 2.1.2 (2 points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "\n",
    "Plot the correlation matrix running the code below. (You may need to zoom on it)\n",
    "What is the relationship between the correlation matrix and the covariance matrix? (1) Check the correct box below and (2) motivate your answer.\n",
    "\n",
    "- [ ] The correlation matrix contains the unnormalized covariance values\n",
    "- [ ] The correlation matrix contains the normalized covariance values\n",
    "- [ ] The covariance matrix contains the variance of the correlation\n",
    "\n",
    "<font color='red'>Do NOT just choose an answer. Please clarify WHY this is the correct answer.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_plot = df.drop(['target'],axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(df_to_plot.corr(),annot=True,linewidths=1, cmap=\"YlGnBu\", annot_kws={\"fontsize\":10}, vmax=1, ax=ax)\n",
    "plt.title('Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-behalf",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-shelter",
   "metadata": {},
   "source": [
    "### Task 2.1.3 (3 points)\n",
    "\n",
    "In this task, we reason about the covariance matrices.\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> code for normalizing the features of the wine dataset using (1) standard score normalization and (2) range normalization. Finally, (3) plot the **covariance** matrices for\n",
    "1. The unnormalized data\n",
    "2. The [standard score normalized features](https://en.wikipedia.org/wiki/Standard_score)\n",
    "3. The range (min-max) normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = data_np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-decision",
   "metadata": {},
   "source": [
    "### Task 2.1.4 (2 points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> how the covariance matrix changes with different normalization schemes and reason on why such behaviour appears.\n",
    "You should notice some differences. (1) Check the correct box below and (2) motivate your answer.\n",
    "\n",
    "\n",
    "\n",
    "- [ ] Range normalization preserves the variance. Therefore, features are directly comparable within the matrix.\n",
    "- [ ] Standard score normalization preserves the variance. Therefore, features are directly comparable within the matrix.\n",
    "- [ ] Both methods normalize in such a way, that it makes sense to compare the different covariance values to each other within the matrix. \n",
    "- [ ] None of the methods normalize in such a way that it makes sense to compare the different covariance values to each other.\n",
    "\n",
    "<font color='red'>Do NOT just choose an answer. Please clarify WHY this is the correct answer.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-adapter",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-entry",
   "metadata": {},
   "source": [
    "## Task 2.2 Normal distribution\n",
    "### Task 2.2.1 (4 points)\n",
    "Sometimes it is convenient to know whether a variable is close to a normal distribution.\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> a method norm_dist that: <br>\n",
    "    \n",
    "1) **Inputs**: \n",
    "    * the number of buckets $b$ \n",
    "    * a vector $x$ of values \n",
    "2) First, compute the histogram of a Gaussian variable with mean $\\mu$ corresponding to the sample mean of $x$ and $\\sigma^2$ corresponding to the sample variance of $x$. Second, calculate the histogram of $x$ using $b$ buckets. \n",
    "3) **Output**: the sum of the absolute differences of the buckets between the two histograms computed in 2). The sum of the differences is computed as \n",
    "$$\\sum_{i=1}^b |H_X(i) - H_{\\mathcal{N}}(i)|$$ \n",
    "where $H_X(i)$ is the i-th bucket of the histogram of $x$ and $H_\\mathcal{N}(i)$ is the i-th bucket of the hisotgram obtained from the normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$. \n",
    "\n",
    "<font color='red'>You can use the norm function from Scipy to get the normal distribution to subtract from.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "## Our data comes from the variable X\n",
    "X = data_np\n",
    "def norm_dist(x, b): \n",
    "    dist = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-mission",
   "metadata": {},
   "source": [
    "### Task 2.2.2 (6 point)\n",
    "A) <span style='color: green'>**\\[Motivate\\]**</span> which drawbacks the method in Task 2.2.1 has. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-sharp",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-blowing",
   "metadata": {},
   "source": [
    "B) <span style='color: green'>**\\[Motivate\\]**</span> whether the method in Task 2.2.1  is robust to outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-blade",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-crown",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Implement\\]**</span><br>\n",
    "C) Run your code on each columns of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-female",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "D) What is the column with the largest distance? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-workplace",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-delay",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "E) Do the features follow a normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-match",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-cyprus",
   "metadata": {},
   "source": [
    "### Task 2.2.3 (2 points)\n",
    "\n",
    "Now look at the method below. This is called a Quantile-Quantile [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot). \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> why this method is more robust than the one we proposed in Task 2.2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-battery",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from matplotlib import gridspec\n",
    "\n",
    "plt.tight_layout()\n",
    "_, n = X.shape\n",
    "print(headers, X.shape)\n",
    "fig = plt.figure(constrained_layout=True, figsize=(8, 50))\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=(n-1), figure=fig)\n",
    "for i in np.arange(2,n):\n",
    "    x = toy[headers[i]]\n",
    "    r = i-1\n",
    "    qq = fig.add_subplot(spec[r, 1]) \n",
    "    stats.probplot(x, plot=qq)\n",
    "    h = fig.add_subplot(spec[r, 0])\n",
    "    h.set_title(headers[i])\n",
    "    h.hist(x, bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-harbor",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "collaborative-kinase",
   "metadata": {},
   "source": [
    "# Part 3 Cluster Analysis\n",
    "In this section, you will perform cluster analysis of the dataset in Part 2 and modify clustering algorithms to achieve better results. \n",
    "\n",
    "## Task 3.1\n",
    "\n",
    "### Task 3.1.1 (6 points)\n",
    "A)  <span style='color: green'>**\\[Implement\\]**</span> and plot the **silhouette coefficient** to detect the number of clusters $k$. \n",
    "\n",
    "<font color='red'>You can use the KMeans implementation from scikit-learn.</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = df\n",
    "first = \"alcohol\"\n",
    "second = \"flavanoids\"\n",
    "X = toy[[first, second]].to_numpy()\n",
    "y = toy['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = toy[[first, second]].to_numpy()\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-bubble",
   "metadata": {},
   "source": [
    "B) <span style='color: green'>**\\[Motivate\\]**</span> your choice of clusters $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-pharmacy",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-mortgage",
   "metadata": {},
   "source": [
    "C) <span style='color: green'>**\\[Implement\\]**</span><br>\n",
    "Run k-means on the dataset X, with the number of clusters detected in the previous exercise.\n",
    "\n",
    "<font color='red'>You can use the KMeans implementation from scikit-learn.</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = toy[[first, second]].to_numpy()\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f7c21-17b1-4571-a03e-a84e2ca48ca1",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span> Did you find better clusters? Are they more separated? Why is it a good/bad idea to use the Silhouette coefficient? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319eec41-78f2-4195-bd32-3d8ff42e432a",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-power",
   "metadata": {},
   "source": [
    "### Task 3.1.2 (3 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> Kernel K-means and the Gaussian Kernel. \n",
    "\n",
    "The Gaussian kernel is defined as in the following equation:\n",
    "\n",
    "$$\n",
    "K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\exp \\left(-\\frac{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|^{2}}{2 \\sigma^{2}}\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "X_scaled = StandardScaler().fit(X_norm).transform(X_norm)\n",
    "\n",
    "y = df[['target']].to_numpy()\n",
    "\n",
    "def gaussian_kernel(x,y, sigma=0.2): \n",
    "    k = 0 \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return k\n",
    "\n",
    "\n",
    "def kernel_kmeans(X, n_clusters, kernel=gaussian_kernel, iters=100, error=0):\n",
    "    # For simplicity use 'init' as initial points for your algorithm\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\",max_iter=1).fit(X)\n",
    "    init = kmeans.cluster_centers_\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "        \n",
    "    ### YOUR CODE HERE\n",
    "    return clusters\n",
    "\n",
    "# clusters = kernel_kmeans(X_scaled, 3)\n",
    "# plt.scatter(X[:, 0], X[:, 1], alpha=0.8, c=clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e186498",
   "metadata": {},
   "source": [
    "### Task 3.1.3 (3 points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Run both kmeans and kernel K-means on the data. \n",
    "Which clustering do you think is better and explain why do you think so? Which one most resemble the ground truth labeling? Under which condition is Kernel K-means with Gaussian Kernel better than K-means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clustering = KMeans(n_clusters=3).fit(X_scaled).labels_\n",
    "gaussian_clustering = kernel_kmeans(X_scaled, 3, kernel=gaussian_kernel)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], alpha=0.8, c=kmeans_clustering)\n",
    "axes[0].set_title(\"K-means\")\n",
    "\n",
    "axes[1].scatter(X[:, 0], X[:, 1], alpha=0.8, c=gaussian_clustering)\n",
    "axes[1].set_title(\"Gaussian K-means\")\n",
    "\n",
    "axes[2].scatter(X[:, 0], X[:, 1], alpha=0.8, c=y)\n",
    "axes[2].set_title(\"Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13f2f2",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a634a",
   "metadata": {},
   "source": [
    "### Task 3.1.4 (4 points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Running the code below, draw two example datasets such that:<br>\n",
    "a). K-means produces clustering better than the one obtained by Gaussian K-means.<br>\n",
    "b). Gaussian K-means produces clustering better than the one obtained by Vanilla K-means.\n",
    "\n",
    "Explain why do you think one clustering is better than the other and give the reason why K-means and Gaussian K-means behave this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can use this tool to generate the data\n",
    "from drawdata import ScatterWidget\n",
    "widget = ScatterWidget()\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33075897",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this code to print data from the tool above\n",
    "if len(widget.data) != 0:\n",
    "    data = widget.data_as_pandas[['x', 'y']][:400].to_numpy()\n",
    "    print(np.array2string(data, precision=0, separator=',', ).replace('\\n', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc865c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can use this code for plotting. It's just a template you can modify it if you want to.\n",
    "\n",
    "# Save your example dataset in the array below\n",
    "X = np.array([])\n",
    "\n",
    "# Use the line below for debugging, so that you don't have to copy paste data every time\n",
    "# X = widget.data_as_pandas[['x', 'y']].to_numpy()[:400]\n",
    "\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "X_scaled = StandardScaler().fit(X_norm).transform(X_norm)\n",
    "\n",
    "n_clusters = 2\n",
    "kmeans_clustering = KMeans(n_clusters=n_clusters, random_state=2137).fit(X_scaled).labels_\n",
    "gaussian_clustering = kernel_kmeans(X_scaled, n_clusters, kernel=gaussian_kernel)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], alpha=0.8, c=kmeans_clustering)\n",
    "axes[0].set_title(f\"K-means\")\n",
    "\n",
    "axes[1].scatter(X[:, 0], X[:, 1], alpha=0.8, c=gaussian_clustering)\n",
    "axes[1].set_title(f\"Gaussian K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f036789",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "transparent-junction",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3.2 Clustering quality\n",
    "\n",
    "### Task 3.2.1 (2 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> **Normalized Mutual Information (NNI)** as a measure for clustering quality.\n",
    "\n",
    "\n",
    "**Hint**: First implement **Entropy** and then **Normalized Mutual Information**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "### C is the clustering obtained by an algorithm and T is the ground truth cluster assignments.\n",
    "\n",
    "def entropy(C):\n",
    "    ### IMPLEMENT\n",
    "    return None\n",
    "\n",
    "\n",
    "def NMI(C, T):\n",
    "    ### IMPLEMENT\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d403e7",
   "metadata": {},
   "source": [
    "Run the code below to measure the quality of clustering obtained by k-means in task 3.1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93345c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Mutual Information: None\n"
     ]
    }
   ],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "y = df[['target']].to_numpy()\n",
    "\n",
    "\n",
    "T = y # Ground-truth clusters\n",
    "C = KMeans(n_clusters=3).fit_predict(X_norm)# Clusters obtained by k-means\n",
    "\n",
    "\n",
    "print(f'Normalized Mutual Information: {NMI(C, T)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941bfce",
   "metadata": {},
   "source": [
    "### Task 3.2.2 (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-soundtrack",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "A) Reason about the measure, is the measure influenced by the size of the clusters?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-convert",
   "metadata": {},
   "source": [
    "******************* \n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-police",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "B) What does the measure capture? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-settle",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0834480",
   "metadata": {},
   "source": [
    "### Task 3.2.3 (4 points)\n",
    "A) <span style='color: green'>**\\[Implement\\]**</span> functions computing **Purity** and **F-measure** of a clustering and a ground truth labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dec601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### C is the clustering obtained by an algorithm and T is the ground truth cluster assignments.\n",
    "\n",
    "\n",
    "def purity(C, T):\n",
    "    ### IMPLEMENT\n",
    "    return None\n",
    "\n",
    "\n",
    "def f_measure(C, T):\n",
    "    ### IMPLEMENT\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5127394",
   "metadata": {},
   "source": [
    "Run the code below to measure the quality of clustering obtained by k-means in task 3.1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "y = df[['target']].to_numpy()\n",
    "\n",
    "\n",
    "T = y # Ground-truth clusters\n",
    "C = KMeans(n_clusters=3).fit_predict(X_norm)# Clusters obtained by k-means\n",
    "\n",
    "\n",
    "print(f'Purity: {purity(C, T)}')\n",
    "print(f'F-measure: {f_measure(C, T)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31d0cb-8a86-4936-9538-75dc77bae7ca",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "B) Reason about the differences in the measures, is one more affected by some the characteristics of the clusters (e.g. size, density, radius)? If so, why? What are the drawbacks and advantages of each measure? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e1919-7d23-4115-b430-3083acf58f7a",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-sector",
   "metadata": {},
   "source": [
    "### Task 3.2.4 (2 points)\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "\n",
    "(1) Check the correct box (or boxes) below and (2) motivate your answers.\n",
    "\n",
    "- [ ] Conditional Entropy is preferable over Entropy because it uses all the points.\n",
    "- [ ] F-measure is preferable over Purity because it is less computational demanding.\n",
    "- [ ] Contingency table is always a square matrix.\n",
    "- [ ] As number of clusters increases Purity tends to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-influence",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "generic-state",
   "metadata": {},
   "source": [
    "## Task 3.3 Gaussian Mixtures and the EM-Algorithm\n",
    "### Task 3.3.1 (4 point)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the EM-algorithm for the Gaussian Mixture Model.\n",
    "<br> You can consult [DMA] Section 13.3.2, for a description of how the algorithm works in this particular setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.gmm import GMM\n",
    "class MyGMM(GMM):\n",
    "    def initialize_parameters(self, X):\n",
    "        \"\"\"\n",
    "            This function should utilize information from the data to initialize\n",
    "            the parameters of the model.\n",
    "            In particular, it should compute initial values for mu, Sigma, and pi.\n",
    "            \n",
    "            The function corresponds to line 2-4 in Algorithm 13.3 in [DMA, p. 349]\n",
    "            Note, that K can be retrieved as `self.K`.\n",
    "\n",
    "            Args:\n",
    "                X (matrix, [n, d]): Data to be used for initialization.\n",
    "\n",
    "            Returns:\n",
    "                Tuple (mu, Sigma, pi), \n",
    "                    mu has size        [K, d]\n",
    "                    Sigma has size     [K, d, d]\n",
    "                    pi has size        [K]\n",
    "        \"\"\"\n",
    "        # TODO: what should the values be for initializing mu, Sigma and pi\n",
    "        return mu, Sigma, pi\n",
    "\n",
    "\n",
    "    def posterior(self, X):\n",
    "        \"\"\"\n",
    "            The E-step of the EM algorithm. \n",
    "            Returns the posterior probability p(Y|X)\n",
    "\n",
    "            This function corresponds to line 8 in Algorithm 13.3 in [DMA, p. 349]\n",
    "            Note, that mean and covariance matrices can be accessed by `self.mu` and `self.Sigma`, respectively.\n",
    "            \n",
    "            Args:\n",
    "                X (matrix, [n,  d]): Data to compute posterior for.\n",
    "\n",
    "            Returns:\n",
    "                Matrix of size        [n, K]\n",
    "        \"\"\"\n",
    "        # TODO: what is the posterior probability?\n",
    "        \n",
    "        return posterior\n",
    "        \n",
    "\n",
    "    def m_step(self, X, P):\n",
    "        \"\"\"\n",
    "            Update the estimates of mu, Sigma, and pi, given the data `X` and the current\n",
    "            posterior probabilities `P`.\n",
    "\n",
    "            This function corresponds to line 10-12 in Algorithm 13.3 and Eqn. (13.11-13) in [DMA, p. 349].\n",
    "            \n",
    "            Args:\n",
    "                X (matrix, [n, d]): Data matrix\n",
    "                P (matrix, [n, K]): The posterior probabilities for the n samples.\n",
    "\n",
    "            Returns:\n",
    "                Tuple (mu, Sigma, pi), \n",
    "                    mu has size        [K, d]\n",
    "                    Sigma has size    [K, d, d]\n",
    "                    pi has size        [K]\n",
    "        \"\"\"\n",
    "        # TODO: what is the values of mu, Sigma, and pi that maximizes the expectation given the posterior?\n",
    "        return  mu_hat, Si_hat, pi_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-wireless",
   "metadata": {},
   "source": [
    "### Task 3.3.2 (4 points)\n",
    "\n",
    "Run both k-means and your EM-algorithm for GaussianMixtures<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "\n",
    "n_clusters = 3\n",
    "kmeans_clustering = KMeans(n_clusters=n_clusters).fit_predict(X_norm)\n",
    "\n",
    "model = MyGMM(K=n_clusters)\n",
    "model.fit(X_norm)\n",
    "gmm_clustering = model.predict(X_norm)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], alpha=0.8, c=kmeans_clustering)\n",
    "axes[0].set_title(f\"K-means\")\n",
    "\n",
    "axes[1].scatter(X[:, 0], X[:, 1], alpha=0.8, c=gmm_clustering)\n",
    "axes[1].set_title(f\"EM Gaussian Mixtures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0ed97",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "A) Can you see the substantial difference between those two clusterings? Explain it.\n",
    "\n",
    "(*if you don't see any difference try running the algorithm again*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd55a97",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d60e5df",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "B) What are the advantages and disadvantages of this approach?<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8058f8",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-culture",
   "metadata": {},
   "source": [
    "# Part 4 Outlier detection\n",
    "In this exercise we will work with outlier detection techniques and analyze their performance on the small dataset. Before starting the exercise, run the code below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-mumbai",
   "metadata": {},
   "source": [
    "## Task 4.1 (DBoutliers)\n",
    "We will now compare two outlier detection techniques.\n",
    "### Task 4.1.1 (2 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> a simple distance-based outlier detector. This is the distance-based outlier detection from the lectures, where a point is considered an outlier if at most a fraction $pi$ of the other points have a distance less of than $eps$ to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBOutliers(X, eps, pi): \n",
    "    outliers = None\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR ENDS CODE HERE\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-somewhere",
   "metadata": {},
   "source": [
    "### Task 4.1.2 (4 points)\n",
    "A) <span style='color: green'>**\\[Implement\\]**</span>\n",
    "DBOutliers requires tuning the parameters eps, pi. Run the code from Task 4.1.1 with different choices of eps, pi \n",
    "\n",
    "**Note** that the data is normalized. Choose two ranges with **at least** 4 values each.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-simulation",
   "metadata": {},
   "source": [
    "B) <span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "\n",
    "**Present** the results  and **discuss** how the results vary with respect to (1) eps and (2) pi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-stanford",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9fb942",
   "metadata": {},
   "source": [
    "## Task 4.2 Isolation Forest (8 points)\n",
    "\n",
    "In this section you will recreate implementation of the Isolation Forest from the original paper: https://ieeexplore.ieee.org/abstract/document/4781136.\n",
    "\n",
    "<span style='color: red'>**Note**: To access the PDF, you must be connected to the university's Wi-Fi or VPN.</span><br>\n",
    "\n",
    "\n",
    "A) <span style='color: green'>**\\[Implement\\]**</span> Fill missing parts in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(n):\n",
    "    \"\"\"\n",
    "    Defined by the equation (1) in the paper.\\\\\n",
    "    Computes the expected path length for a given tree size based on the average depth of\n",
    "    a randomly generated binary search tree.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n (int): The number of data points in the node.\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    float: The expected path length.\n",
    "    \"\"\"\n",
    "    if n <= 1:\n",
    "        return 0\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    \n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    \n",
    "\n",
    "\n",
    "class IsolationTree:\n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.size = 0\n",
    "\n",
    "\n",
    "    def fit(self, X, e, l):\n",
    "        \"\"\"\n",
    "        Defined by the Algorithm 2 in the paper.\\\\\n",
    "        Trains the tree on data X.\\\\\n",
    "        It's already implemented for you. Do not modify it.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array-like, shape (n_samples, n_features)\n",
    "        e: current tree height\n",
    "        l: height limit\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        IsolationTree: itself\n",
    "        \"\"\"\n",
    "        self.size = len(X)\n",
    "        \n",
    "        if e >= l or self.size <= 1:\n",
    "            return self\n",
    "        \n",
    "        # Choose a random feature and split value\n",
    "        self.split_feature = np.random.randint(X.shape[1])\n",
    "        min_val, max_val = X[:, self.split_feature].min(), X[:, self.split_feature].max()\n",
    "        \n",
    "        if min_val == max_val:\n",
    "            return self\n",
    "        \n",
    "        self.split_value = np.random.uniform(min_val, max_val)\n",
    "        \n",
    "        left_mask = X[:, self.split_feature] < self.split_value\n",
    "        X_left, X_right = X[left_mask], X[~left_mask]\n",
    "        \n",
    "        self.left = IsolationTree().fit(X_left, e + 1, l)\n",
    "        self.right = IsolationTree().fit(X_right, e + 1, l)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def is_external_node(self):\n",
    "        \"\"\"\n",
    "        Checks whether the tree is an external node.\\\\\n",
    "        For the definition of external node ctr+F 'external-node' in the paper.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        Boolean: True is the tree is an external node, otherwise, False.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE\n",
    "    \n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "        \n",
    "    \n",
    "\n",
    "    def path_length(self, x, e=0):\n",
    "        \"\"\"\n",
    "        Defined by the Algorithm 3 in the paper.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: array-like, shape (n_features,)\n",
    "        e: current tree height\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        int: path length of x\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "class IsolationForest:\n",
    "    def __init__(self, n_trees, subsample_size):\n",
    "        self.n_trees = n_trees\n",
    "        self.subsample_size = subsample_size\n",
    "        self.height_limit = np.ceil(np.log2(self.subsample_size))\n",
    "        self.trees = []\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Trains the forest on data X.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array-like, shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            sample = X[np.random.choice(X.shape[0], self.subsample_size, replace=False)]\n",
    "            tree = IsolationTree().fit(sample, 0, self.height_limit)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "\n",
    "    def anomaly_score(self, x):\n",
    "        \"\"\"\n",
    "        Defined by equation (2) in the paper.\\\\\n",
    "        Computes the anomaly score of instance x.\n",
    "        \n",
    "        Note:\n",
    "        -----------\n",
    "        Keep in mind that variable 'n' in the equation in the paper does not represent the size of X but rather the size of a tree i.e. sub-sampling size.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: array-like, shape (n_features,)\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        float: the anomaly score of instance x\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "    def identify_outliers(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Identifies outliers in the X based on an anomaly score.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        threshold: the cutoff value for determining whether a sample is an outlier.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        outliers : array-like, shape (n_samples,)\\\\\n",
    "        A boolean array where each element corresponds to a sample in X.\\\\\n",
    "        True indicates that the anomaly score of the sample is greater than the threshold.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c638507",
   "metadata": {},
   "source": [
    "B). <span style='color: green'>**\\[Implement\\]**</span> Create an Isolation Forest with $n\\_trees = 100$, $subsample\\_size = 50$ and train it on $X$. Then, use the Isolation Forest to find outliers in $X$ and plot the result. You might need to fine-tune parameter $threshold$ so that the result is meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b39e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "\n",
    "### YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b2200-c009-425d-8ea5-b5095e3ece7e",
   "metadata": {},
   "source": [
    "C) <span style='color: green'>**\\[Motivate\\]**</span> Present the results and discuss what you found and what is the impact of $threshold$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed7a55c-3fb1-4579-b6f0-2f1178829e42",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1201f-54cb-46de-afdd-4bf2f4338264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
