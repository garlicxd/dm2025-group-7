{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "threaded-horizon",
   "metadata": {},
   "source": [
    "# Data Mining - Handin 2 - Graph mining\n",
    "\n",
    "This handin corresponds to the topics in Week 11-15 in the course.\n",
    "\n",
    "The handin is \n",
    "* done in groups\n",
    "* worth 10% of the grade\n",
    "\n",
    "For the handin, you will prepare a report in PDF format, by exporting the Jupyter notebook. \n",
    "Please submit\n",
    "1. The jupyter notebook file with your answers\n",
    "2. The PDF obtained by exporting the jupyter notebook\n",
    "\n",
    "**The grading system**: Tasks are assigned a number of points based on the difficulty and time to solve it. The sum of\n",
    "the number of points is **100**. For the maximum grade you need to get at least _90 points_. The minimum grade (02 in the Danish scale)\n",
    "requires **at least** 33 points, with at least 9 points from the first three Parts (Part 1,2,3) and 6 points in the last part (Part 4).\n",
    "Good luck!\n",
    "\n",
    "**The exercise types**: There are four different types of exercises\n",
    "1. <span style='color: green'>**\\[Compute by hand\\]**</span> means that you should provide NO code, but show the main steps to reach the result (not all). \n",
    "2. <span style='color: green'>**\\[Motivate\\]**</span> means to provide a short answer of 1-5 lines indicating the main reasoning, e.g., the PageRank of a complete graph is 1/n in all nodes as all nodes are symmetric and are connected one another.\n",
    "3. <span style='color: green'>**\\[Prove\\]**</span> means to provide a formal argument and NO code. \n",
    "4. <span style='color: green'>**\\[Implement\\]**</span> means to provide an implementation. Unless otherwise specified, you are allowed to use helper functions (e.g., ```np.mean```, ```itertools.combinations```, and so on). **However**, if the task is to implement an algorithm, by no means a call to a library that implements the same algorithm will be deemed as sufficient!\n",
    "\n",
    "**Q&A**\n",
    "\n",
    "Q: If the task is to implement a mean function, may I just call ```np.mean()```? \n",
    "<br>A: No.\n",
    "\n",
    "Q: If the task is to compare the mean of X and Y, may I use ```np.mean()``` to calculate the mean?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: If I have implemented a mean function in a previous task, but I am unsure of its correctness, may I use ```np.mean()``` in following task where mean is used as a helper function? \n",
    "<br>A: Yes.\n",
    "\n",
    "Q: May I use ```np.mean()``` to debug my implementation of mean?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Do I get 0 points for a task if I skip it?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Can I get partial points for a task I did partially correct?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Is it OK to skip a task if I do not need the points from it?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Should I inform a TA if I find an error?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Should I ask questions if I am confused?\n",
    "<br>A: Yes.\n",
    "\n",
    "\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5368d",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">New packages have been added to \"requirements.yml\". To install them, go to the root of the repository and run:\n",
    "```\n",
    "conda activate dm25\n",
    "conda env update -f requirements.yml --prune\n",
    "```\n",
    "</span>\n",
    "\n",
    "<span style=\"color:red\">or install them manually:\n",
    "```\n",
    "pip install networkx\n",
    "pip install torch\n",
    "pip install torchvision\n",
    "```\n",
    "</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b7c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN IMPORTS - DO NOT TOUCH!\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#!{sys.executable} -m pip install networkx\n",
    "#!{sys.executable} -m pip install torch\n",
    "#!{sys.executable} -m pip install torchvision\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import csv\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "### END IMPORTS - DO NOT TOUCH!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-degree",
   "metadata": {},
   "source": [
    "## Task 1.1 Random walks and PageRank  (10 points)\n",
    "\n",
    "In this exercise recall that the PageRank is defined as \n",
    "$$\\mathbf{r} = \\alpha \\mathbf{Mr} + (1-\\alpha)\\mathbf{p}$$ \n",
    "where $\\mathbf{r}\\in \\mathbb{R}^n$ is the PageRank vector, $\\alpha$ is the restart probability, $\\mathbf{M} = A\\Delta^{-1}$, and $\\mathbf{p}$ is the restart (or personalization) vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-arthritis",
   "metadata": {},
   "source": [
    "### Task 1.1.1 (2 points)\n",
    "What is the PageRank of a **$d$-regular** graph with $n$ nodes and $\\alpha=1$? \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> your answer without showing the exact computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-invasion",
   "metadata": {},
   "source": [
    "****\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "\n",
    "## Answer task 1.1.1\n",
    "\n",
    "Given that alpha is equal to 1, we can observe that the page rank formula becomes  $r = Mr$.\n",
    "We do know that each node has exactly the same number of neighbors, given that that's the definiton of a d-regular graph. Having all nodes share the same numbers of neighbors means that the probability of reaching each node is equal for all nodes, therefore their importance is equally distributed, meaning that the PageRank  of this graph is of [$\\frac{1}{n}$, $\\frac{1}{n}$, ... , $\\frac{1}{n}$]. \n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-subscriber",
   "metadata": {},
   "source": [
    "### Task 1.1.2 (6 points)\n",
    "Look at the graph below (run the code) and try to make a guess about the PageRank values of each node by only looking at the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edges_from([(1,2),(2,3), (2,4), (3,4), (1,3)])\n",
    "nx.draw(G, with_labels=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-reproduction",
   "metadata": {},
   "source": [
    "<span style='color: green'>**A) \\[Implement\\]**</span> the PageRank for $\\alpha=1$ for the graph using the Power Iteration method (use $\\epsilon=1e-16$ to stop the iteration).<br> \n",
    "\n",
    "<span style='color: green'>**B) \\[Implement]**</span> Plot the norm square difference of the $r$ vector between two iterations. \n",
    "\n",
    "<span style='color: green'>**C) \\[Motivate\\]**</span> Do you observe a constanct decrease of the norm square difference as iterations are increasing, and is it expected or not?\n",
    "\n",
    "<span style='color: green'>**D) \\[Implement\\]**</span> the PageRank for $\\alpha=1$ using the eigenvector method.<br> \n",
    "\n",
    "<span style='color: green'>**E) \\[Motivate\\]**</span> Are solutions of both methods the same? Why don't we only use the eigenvector method that optimally solves the problem? \n",
    "\n",
    "<span style='color: green'>**F) \\[Motivate\\]**</span> Do the real eigenvalues match with your first guess? Can you see a pattern between the eigenvalues of each node and its edges?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A) YOUR CODE HERE \n",
    "def my_pagerank(G, alpha=1, p=None, max_iter=100, tol=1e-06):\n",
    "    A = nx.adjacency_matrix(G).astype(float).todense()\n",
    "    n = G.number_of_nodes()\n",
    "    r = np.full(n, 1/n)\n",
    "\n",
    "    if p is None:\n",
    "        p = np.full(n, 1/n)\n",
    "\n",
    "    Deg_Matrix = np.sum(A, axis=1).flatten()\n",
    "    Deg_Matrix[Deg_Matrix == 0] = 1\n",
    "    M = (A / Deg_Matrix[:, None]).T  \n",
    "\n",
    "    norm_square_difference_list = []\n",
    "    for _ in range(max_iter):\n",
    "        new_r = alpha * M @ r + (1 - alpha) * p\n",
    "        norm_square = np.linalg.norm(new_r - r) ** 2\n",
    "        norm_square_difference_list.append(norm_square)\n",
    "        if np.allclose(new_r, r, atol=tol):\n",
    "            break\n",
    "        r = new_r\n",
    "\n",
    "    return r, norm_square_difference_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#B) YOUR CODE HERE \n",
    "def plot_covergence(norm_square_difference_list):\n",
    "    plt.plot(norm_square_difference_list, label='Norm square difference', marker='o')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Norm square difference')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title('Convergence of PageRank')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "r, norm_diff = my_pagerank(G)\n",
    "plot_covergence(norm_diff)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corrected-reward",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> C) YOUR ANSWER HERE</span>\n",
    "\n",
    "***\n",
    "The decrease is continously logaritmic. This is due to the iterative repeating multiplication of M for each termporal r. The matrix M represents the eigenvalues, having the value less than 1, meaning exponential decrease per iteration, therefore the difference between the current value of r and the next iteration r+1 is exponentially decreasing, therefore you see a contionus decreasing given the y axis being logaritmic. \n",
    "***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D) YOUR CODE HERE\n",
    "\n",
    "def eigenvectorPageRank(G, alpha=1, p=None,  max_iter=100, tol=1e-06):\n",
    "    A = nx.adjacency_matrix(G).astype(float).todense()\n",
    "    n = G.number_of_nodes()\n",
    "\n",
    "    if p is None:\n",
    "        p = np.full(n, 1/n)\n",
    "\n",
    "    Deg_Matrix = np.sum(A, axis=1).flatten()\n",
    "    Deg_Matrix[Deg_Matrix == 0] = 1\n",
    "    M = (A / Deg_Matrix[:, None]).T\n",
    "\n",
    "    G_matrix = alpha * M + (1 - alpha) * p[:, None]\n",
    "\n",
    "    eigvals, eigvecs = np.linalg.eig(G_matrix)\n",
    "\n",
    "    idx = np.argmax(np.abs(eigvals))\n",
    "\n",
    "    dominant_eigvec = np.abs(eigvecs[:, idx]).flatten()\n",
    "\n",
    "    pagerank_vector = dominant_eigvec / np.sum(dominant_eigvec)\n",
    "\n",
    "\n",
    "    return pagerank_vector, eigvals, eigvecs\n",
    "\n",
    "\n",
    "\n",
    "pageRank_vector, eigvals, eigvecs = eigenvectorPageRank(G)\n",
    "differences = np.abs(r - pageRank_vector)\n",
    "print(\"Differences between power iteration and eigenvector method: \", differences)\n",
    "print()\n",
    "print(\"Eigvals of the matrix: \", eigvals)\n",
    "print(\"Eigvectors of the matrix: \", eigvecs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-token",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> E) YOUR ANSWER HERE</span>\n",
    "\n",
    "***\n",
    "The solutions, as seen in the difference variable being printed out, are relatively the same, very similar, therefore the problem is not the result, but more about the procedure in order to get the result. The eigenvector method  requires to compute all the eigenvalues and eigenvectors of a  matrix, therefore approx a O(n^3) time complexity, much more less efficient than the Power method, especially given complex graphs. \n",
    "Another problem, also pointed in the chapter 5 of the MMD book, is that the matrix M can be very sparse, and the eigenvector decomposition converts it into a dense format, a lot of computational power extra required for such task. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-reporter",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> F) YOUR ANSWER HERE</span>\n",
    "\n",
    "***\n",
    "Well, \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-campus",
   "metadata": {},
   "source": [
    "### Task 1.1.3 (2 points)\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Assume you have embedded the graph in **1.1.2** with a __Linear Embedding__ using unnormalized Laplacian matrix of the graph as the similarity matrix. How do you expect the embeddings to be if the embedding dimension is $d = 1$? \n",
    "\n",
    "* [ ] Nodes 1, 2, 3, 4 will be placed in the corners of a hypercube\n",
    "* [X] Nodes 2,3 will have the same embedding while 1,4 will be far from each other.\n",
    "* [ ] Nodes 1,4 have the same embedding and 2,3 will have very close embeddings.\n",
    "* [ ] Nodes 3,4 will be very far apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-auckland",
   "metadata": {},
   "source": [
    "****\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************\n",
    "\n",
    "***\n",
    "Well, we observe the initial graph and observe that node 1 and 4 have two connections, degree of 2, whereas 2 and 3 have degree of 3. Regarding adjacency, we observe that both 2 and 3 have connections to ALL nodes, whereas 1 and 4 have connection to 2 and 3, no connections between each other. Regarding the missing connectivity between 1 and 4, while 2, 3 are connected to all nodes, the correct answer is> \" Nodes 2,3 will have the same embedding while 1,4 will be far from each other.\"\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-tension",
   "metadata": {},
   "source": [
    "## Task 1.2: Spectral Properties of the Graph Laplacian (10 points)\n",
    " <span style='color: green'>**\\[Prove\\]**</span> **the following properties:**\n",
    " You will be given points for each of the properties that you prove, rather than points for the exercise as a whole.\n",
    "\n",
    "**Note that all question correspond to the eigenvalues of the LAPLACIAN (NOT THE NORMALIZED)**\n",
    "\n",
    "For a graph with $n$ nodes the eigenvalues of the LAPLACIAN ($L  = D - A$) is noted as:<br>\n",
    "**$\\lambda_0\\leq\\lambda_1\\leq...\\leq\\lambda_{n-1}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-hybrid",
   "metadata": {},
   "source": [
    "### Task 1.2.1 (2 points)\n",
    "\n",
    "For all graphs $\\lambda_0 = 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-lying",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************\n",
    "\n",
    "***\n",
    "We know per theorem that all eigenvalues are greater than 0, therefore we know that $\\lambda_{0} \\geq 0$.  Well, we know that, for any graph, no matter the graph, there exists a eigenvalue having the value 0, due to eigenvectors such as the eigenvector 1.  \n",
    "More concrete, given formula $L \\cdot x = \\lambda \\cdot  x$, where x is eigenvector, $\\lambda$ being eigenvalue and L being the Laplacian. So, given $x = 1$, we have:  $\\lambda = L \\cdot 1 = 0$, due to having the nodes degree added with the negative value of -1 per edge connection for nodes, add because the adjacency added togheter per node is equivalent with the degree of that specific node, you get 0, having $\\lambda = 0$ if $x=1$, and since 0 is the absolute smallest value of en eigenvalue and the smallest eigenvalue is $\\lambda_{0}$, we know $\\lambda_{0} = 0 $\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-jaguar",
   "metadata": {},
   "source": [
    "### Task 1.2.2 (2 points)\n",
    "For the complete graph, $\\lambda_1, \\dots, \\lambda_{n-1} = n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-pattern",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************\n",
    "\n",
    "***\n",
    "Let's first analyse and understand what a complete graph is that when all nodes are connected with each other, meaning that the matrix L = D - A  will have it's diagonal equal to the degree, while the rest of the entries are all equal to one. The diagonal equal to the degree, given a complete graph, is n-1 (not include yourself).  Let's first look at the classic eigenvalue equation: $Lx = \\lambda{x}$. Regarding the previous proof for $\\lambda_{0} = 0$ where the eigenvector is $[111...11]^{T}$, we need a vector being orthogonal to the 1 vector in order to respect the eigenvalue equation. Orthogonality means that the dot product between our vector $[x_{1}x_{2}...x_{n}]^{T}$ and $[111...11]^{T}$ should be 0. The only possibility for our vector to be 0 is that $\\sum_{i} x_{i}=0$. \n",
    "\n",
    "Let's define $z = Lx$, where $z = \\lambda x$. Well, given the value z, for the i'th entry is equal to i'th row of L with y. Well, we know that the diagonal  is (n-1), the i'th position, and the rest are -1, meaning:\n",
    "\n",
    "$z_{i} = (n-1)x_{i} - \\sum_{j \\neq i}x_{j}$\n",
    "\n",
    "Simplifing to\n",
    "\n",
    "$z_{i} =  (nx_{i} - x_{i}) - \\sum_{j \\neq i}x_{j}$ = $nx_{i} - (x_{i} + \\sum_{j \\neq i}x_{j})$ = $nx_{i} - \\sum_{j=1}^{n}x_{j}$ = $nx_{i}$\n",
    "\n",
    "That's because of the orthogonality rule which says $\\sum_{i} x_{i} = 0$\n",
    "\n",
    "More concrete, we have $z_{i} = nx_{i}$ generalized to Lx = nx, where $\\lambda = n$, meaning that $\\lambda_{1}, ... , \\lambda_{n-1} = n$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-spare",
   "metadata": {},
   "source": [
    "### Task 1.2.3 (2 points)\n",
    "\n",
    "For all the graphs with $k$ connected components $\\lambda_0 = \\lambda_1 =...=\\lambda_ k = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-piano",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************\n",
    "\n",
    "***\n",
    "Well, we know that each node should have a degree of at least k in order to have the graph being k connected, as well as removing k-1 vertices do not disconnect the graph. To get $lambda = 0$, we need to have a constant eigenvector, as showm in Task 1.2.1, but that doesn't show specificlaly that $\\lambda_{0} = ... = \\lambda_{k} = 0$, but only shows that there is at least one lambda being equal to 0. In order to have specifically $\\lambda_{0} = ... = \\lambda_{k} = 0$, but we need to make sure that k of those constant eigenvectors are linearly independent. This can be done by looking locally for each component, where each component do have a number of connections (for example, one component can have 3 connections, meaning degree of 3 and all connected with each other, another can have 2, and so on, until k components). For those, we define eigenvectors [1 1 1] and [1 1] locally (given 3 and 2 connected component). More concrete, if locally looking at the components, we define 1 vectors for those that have the size of the nodes being interconnected locally. Given globally observation where we have n nodes, not only component group number of nodes, we have the same eigenvectors, but we fill the rest of the entries to be 0, having the value globally being $\\lambda = 0$. Since we have now k independent eigenvectors with this procedure, we got $\\lambda_{0} = \\lambda_{1} = ... = \\lambda_{k} = 0$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-private",
   "metadata": {},
   "source": [
    "### Task 1.2.4 (2 points)\n",
    "Given a graph $G$ with eigenvalues of the laplacian $\\lambda_0, \\lambda_1,...,\\lambda_{n-1}$.<br>\n",
    "We remove a single edge from $G$ and we re-calculate the eigenvalues as $\\lambda'_0, \\lambda'_1,...,\\lambda'_{n-1}$.<br>\n",
    "\n",
    "Can we have $\\lambda'_{i}>\\lambda_{i}$ for some $0\\leq i\\leq n-1$? Why? Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-strengthening",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "*****************\n",
    "***\n",
    "For this exercise, we use Rayleigh Theorem, where we focus on $\\sum_{(i,j)\\in E} (x_{i} - x_{j})^{2} = x^{T}Lx$. Given that we have in the new grah $G^{'}$ an edge less than G, we conclude that $x^{T}L'x \\leq x^{T}Lx$ , because  the summation of squared distance between all edges is less or equal (could have been 0 for the original graph G) for the new Graph, given a edge less. Regarding the notebook where $\\lambda_{n} = min_{x \\neq 0} \\frac{m^{T}Mx}{x^{T}x}$ and $\\lambda_{n-1} = min_{x \\neq 0, x^{T}x_{1} = 0} \\frac{x^{T}Mx}{x^{T}x}$ we can addapt it to our laplace L for any lambda i, and get:  $\\lambda_{i} = \\lambda_{n-(n-i)} = min_{x \\neq 0, x^{T}x_{1} , ... , x^{T}x_{n-i}} \\frac{x^{T}Lx}{x^{T}x} $. From this we can conclude that, since x is an arbitrary equivalent vector in both cases, where the only significant difference is the value of the Rayleigh Theorem, where given $x^{T}L'x \\leq x^{T}Lx$, we conclude that, regardless of lambda, $\\lambda_{i}' \\leq \\lambda_{i}$. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exterior-anatomy",
   "metadata": {},
   "source": [
    "### Task 1.2.5 (2 points)\n",
    "Suppose that the graph $G$ is consisted of two connected componentes of equal size named as $G_1$ and $G_2$.<br>\n",
    "The Laplacian of $G_1$ has eigenvalues $\\lambda^1_0,\\lambda^1_1,...,\\lambda^1_{n/2-1}$.<br>\n",
    "The Laplacian of $G_2$ has eigenvalues $\\lambda^2_0,\\lambda^2_1,...,\\lambda^2_{n/2-1}$.<br>\n",
    "Prove that the  Laplacian of $G$ is consisted of the eigenvalues of the Laplacians of $G_1$ and $G_2$ in ascending order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-account",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "******************\n",
    "***\n",
    "Given two components, we can evaluate each individual component and find the eigenvectors accordinlgy for the eigenvalues. Given L_{G1} being the laplacian of our graph $G_{1}$, it's eigenvalue of type $\\lambda_{k}^{1}$ is $v_{k}$, respectively $\\lambda_{k}^{2}$ and eigenvector $w_{k}$ for $G_{2}$. If the graphs $G_{1}$ and $G_{2}$ are disjointly part of graph G, then we have can also merge the eigenvalues by having first the eigenvectors for the first graph in form $(v, 0)$, influencing only the elements from the laplacian of the first graph, and $(0, w)$ incluencing only the elements from the laplacian of the second graph. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-sewing",
   "metadata": {},
   "source": [
    "# Part 2: Graphs and Spectral clustering\n",
    "In this part, you will experiment and reflect on spectral clustering as a technique for partitioning a graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-airplane",
   "metadata": {},
   "source": [
    "## Task 2.1 $\\varepsilon$-neighbourhood graph (14 points)\n",
    "\n",
    "In this subsection you will experiment with biological data https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003268.\n",
    "\n",
    "First run the following code to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "from utilities.make_graphs import read_edge_list, read_list, load_data\n",
    "import numpy as np\n",
    "X, Y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-suspension",
   "metadata": {},
   "source": [
    "******************\n",
    "### Task 2.1.1 (4 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the $\\varepsilon$-neighborhood graph, using Euclidean (L2) distance. \n",
    "\n",
    "**Note**: Be sure that your constructed graph does not contain loop edges (edges from i to i for some node i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "# Be sure that your constructed graph does not \n",
    "# contain loop edges (edges from i to i for some node i)\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "\n",
    "def nn_graph(data, eps, remove_self=True, directed=False):\n",
    "    n = len(X)\n",
    "    G = nx.Graph()    \n",
    "    if directed:\n",
    "        G = nx.DiGraph()\n",
    "    ### YOUR CODE HERE\n",
    "    dataList = [list(arr) for arr in data]\n",
    "    dataList = np.array(dataList)\n",
    "\n",
    "    # print(\"dataList shape is \", dataList.shape)\n",
    "    # print( \"dataList is \", dataList)\n",
    "    # print(\"G is \", G)\n",
    "\n",
    "    G.add_nodes_from(range(len(dataList)))\n",
    "    for i in range(len(dataList)):\n",
    "        current_i = i + 1\n",
    "        for j in range(current_i, len(dataList)):\n",
    "            if i != j:\n",
    "                dist = np.linalg.norm(dataList[i] - dataList[j])\n",
    "                if dist < eps:\n",
    "                    G.add_edge(i, j)\n",
    "    \n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return G\n",
    "\n",
    "eps = 0.01\n",
    "G = nn_graph(X, eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-light",
   "metadata": {},
   "source": [
    "### Task 2.1.2 (2 points)\n",
    "\n",
    "\n",
    "Try with different epsilons (select a small set of epsilons, e.g., 0.01-0.5 values) and plot the graphs. \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> what you observe as epsilon increases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run the code below\n",
    "eps_values = [0.01, 0.05, 0.1, 0.2, 0.4]\n",
    "\n",
    "for eps in eps_values: \n",
    "    ax=plt.subplot()\n",
    "    ax1=plt.subplot()\n",
    "    G = nn_graph(X, eps)\n",
    "    #G1=nx.numpyto\n",
    "    pos=nx.spring_layout(G)  \n",
    "    nx.draw_networkx_edges(G,pos=X)\n",
    "    nx.draw_networkx_nodes(G, pos=X, node_color=Y, node_size=20, cmap=plt.get_cmap('tab10'))\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-slovak",
   "metadata": {},
   "source": [
    "****\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "\n",
    "\n",
    "The epsilon-neighbor algorithm is only conserned on connecting the nodes that are close within epsilon in an euclidean space, given that each of them have a coordinate. Given epsilon increases, it means that we allow even further away nodes to connect with each other, therefore we expect more and more connections until all points are connected with each other. \n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d27cbf",
   "metadata": {},
   "source": [
    "### Task 2.1.3 (4 points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> a k-NN graph and compare with the $\\varepsilon$-neighborhood graph. \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Under what conditions are they similar? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knn_graph(data, k, remove_self=True, directed=False):\n",
    "    nn = NearestNeighbors(n_neighbors=k-1)  \n",
    "    nn.fit(data)\n",
    "    indices = nn.kneighbors(data, return_distance=False)  \n",
    "    ##YOUR CODE HERE \n",
    "\n",
    "    G = nx.DiGraph() if directed else nx.Graph()\n",
    "\n",
    "    \n",
    "    G.add_nodes_from(range(len(data)))\n",
    "\n",
    "    \n",
    "    added_edges = set()\n",
    "\n",
    "    \n",
    "    for i, neighbors in enumerate(indices):\n",
    "        for j in neighbors:\n",
    "            if remove_self and i == j:\n",
    "                continue  \n",
    "\n",
    "            if directed:\n",
    "                G.add_edge(i, j)\n",
    "            else:\n",
    "                edge = tuple(sorted((i, j)))\n",
    "                if edge not in added_edges:\n",
    "                    G.add_edge(i, j)\n",
    "                    added_edges.add(edge)\n",
    "\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "#YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "G = knn_graph(X, 5)\n",
    "pos=nx.spring_layout(G)  \n",
    "nx.draw_networkx_edges(G,pos=X)\n",
    "nx.draw_networkx_nodes(G, pos=X, node_color=Y, node_size=20, cmap=plt.get_cmap('tab10'))\n",
    "ax.set_xlim(-0.1, 1.1)\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f90218-dc5c-4259-8675-1f224a4b3adf",
   "metadata": {},
   "source": [
    "****\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "\n",
    "Well, the difference between the epsilon-neighbors and k-NN is that nodes create edges totally differently, where the nodes from the epsilon-neighbors can have different degrees to whom they are connected to, whereas k-NN nodes do have a fixed and rigid degree, where the edges do not reflect nodes of distances withing a threshold.\n",
    "\n",
    "One condition where both are similar is if the threshold distance in epsilon-neighbors algorithm is high enough to have all the nodes being connected with each other (distance = distance between the furthest two points from our graph), and having n-1 neighbors in the k-nn algorithm.\n",
    "\n",
    "Another condition is given visible convex dense clusters, having a specific number of points per each cluster being relatively close to each other. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-groove",
   "metadata": {},
   "source": [
    "### Task 2.1.4 (2 points)\n",
    "Assign to each edge in the $\\varepsilon$-neighborhood graph a weight\n",
    "\n",
    "$$W_{i j}=e^{-\\frac{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|^{2}}{t}}$$\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the function ```weighted_nn_graph``` below that returns the weighted graph given the data matrix in input and the values eps and t, where t is the parameter of the equation above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_nn_graph(data, eps=20, t=0.1):\n",
    "    n = len(data)\n",
    "    G = nx.Graph()\n",
    "    ### YOUR CODE HERE\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dist = np.linalg.norm(data[i] - data[j])\n",
    "            if dist  <= eps:\n",
    "                weight = np.exp(-dist**2 / t)\n",
    "                G.add_edge(i, j, weight=weight)\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-venezuela",
   "metadata": {},
   "source": [
    "### Task 2.1.5 (2 points)\n",
    "\n",
    "We now vary $t \\in \\{10, 0.1, 0.000001\\}$. Plot the weights as a histogram in order to analyse the results using the provided code.</br>\n",
    "What happens when $t$ is very small, close to $0$, i.e., $t \\rightarrow 0$?</br> What happens when $t$ is very large?\n",
    "</br>Is the behaviour with $t = 0$ expected?\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> your answer reasoning on the formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [10, 0.2, 0.07, 0.000001]\n",
    "fig, ax = plt.subplots(1,4, figsize=(20, 10))\n",
    "row = 0\n",
    "\n",
    "for i, t in enumerate(ts):\n",
    "    G = weighted_nn_graph(X, eps=60, t=t)\n",
    "    ys = []\n",
    "    \n",
    "\n",
    "    col = i \n",
    "    for i, d in enumerate(G.edges.data()):\n",
    "        ys.append(d[2]['weight'])\n",
    "    plt.hist(ys, bins=100)\n",
    "    ax[col].hist(ys, bins=100)\n",
    "    ax[col].set_title(\"t: \"+str(t))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-adolescent",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "\n",
    "Ok, let's answer each question individually.  Regarding t very small, like very close to 0, we observe that there are way more 0 weight entries than the other from 1. That also makes sense, due to the fact that we take the epsilon of a very big negative number, we lend more towards 0, that's why the argument. \n",
    "\n",
    "Given that t is very large, like t=10, we can observe that the the weight frequency is distributed much more on the high end, where no many weights are smaller than 0.88. The frequencies are more distributed given t being around 0.2, for example. That' because we have the inner paranthesis being closer and closer to 0, where exp(0) = 1\n",
    " \n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-stopping",
   "metadata": {},
   "source": [
    "## Task 2.2: Spectral clustering (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-munich",
   "metadata": {},
   "source": [
    "### Task 2.2.1 (2 points)\n",
    "Compute the eigenvectors and eigenvalues (using the provided function) of the Normalized Laplacian and the Random Walk Laplacian of the graph $G$.<br> \n",
    "Plot the spectrum (eigenvalues).\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the code to compute the different Laplacians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_eig(L): \n",
    "    \"\"\"\n",
    "        Takes a graph Laplacian and returns sorted the eigenvalues and vectors.\n",
    "    \"\"\"\n",
    "    lambdas, eigenvectors = np.linalg.eig(L)\n",
    "    lambdas = np.real(lambdas)\n",
    "    eigenvectors = np.real(eigenvectors)\n",
    "    \n",
    "    order = np.argsort(lambdas)\n",
    "    lambdas = lambdas[order]\n",
    "    eigenvectors = eigenvectors[:, order]\n",
    "    \n",
    "    return lambdas, eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_norm = None\n",
    "L_rw = None\n",
    "nodeID = pickle.load(open('./data/nodeID.pickle', 'rb'))\n",
    "A = nx.adjacency_matrix(G).toarray()  # Step 1: Adjacency matrix\n",
    "n = A.shape[0]                        # Number of nodes\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Degree matrix\n",
    "D = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    D[i, i] = np.sum(A[i])           # degree of node i\n",
    "\n",
    "# Step 3: Random Walk Laplacian: L_rw = I - D^-1 * A\n",
    "L_rw = np.identity(n)\n",
    "for i in range(n):\n",
    "    if D[i, i] != 0:\n",
    "        for j in range(n):\n",
    "            L_rw[i, j] -= A[i, j] / D[i, i]\n",
    "    # else: skip row entirely (node i has degree 0)\n",
    "\n",
    "# Step 4: Normalized Laplacian: L_norm = I - D^-1/2 * A * D^-1/2\n",
    "L_norm = np.identity(n)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if D[i, i] != 0 and D[j, j] != 0:\n",
    "            L_norm[i, j] -= A[i, j] / (np.sqrt(D[i, i]) * np.sqrt(D[j, j]))\n",
    "        # else: skip if either degree is 0 (isolated node)\n",
    "\n",
    "eigval_norm, eigvec_norm = graph_eig(L_norm)\n",
    "eigval_rw, eigvec_rw = graph_eig(L_rw)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.plot(eigval_norm, 'b-o', label='Spectrum of Normalized Laplacian', )\n",
    "plt.legend()\n",
    "plt.figure(1)\n",
    "plt.plot(eigval_rw, 'b-o', label='Spectrum of the Random Walk Laplacian')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-release",
   "metadata": {},
   "source": [
    "### Task 2.2.2 (5 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the function ```spect_cluster``` that returns a vector ```y_clust``` in which each entry y_clust\\[i\\] represents the community assigned to node $i$. The method should be able to handle both the Normalized Laplacian, and the Random Walk Laplacian. You are allowed to use your implementation from the weekly exercises and ```sklearn.cluster.k_means``` for k-means clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19811963",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gml('./data/football.gml')\n",
    "groundTruth = [data['gt'] for _, data in G.nodes(data=True)]\n",
    "\n",
    "\n",
    "fig = plt.figure(1, figsize=(15, 15), dpi=60)\n",
    "layout = nx.kamada_kawai_layout(G)\n",
    "nx.draw_networkx_edges(G, layout, alpha=0.2)\n",
    "nx.draw_networkx_nodes(G, layout, node_color=groundTruth, node_size=400)\n",
    "nx.draw_networkx_labels(G, layout, font_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-composite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import k_means\n",
    "\n",
    "def spect_cluster(G, eig_type=\"normal\", k=5, d=5):\n",
    "    ### YOUR CODE HERE\n",
    "    # Copy pase from the previous exercise\n",
    "    A = nx.adjacency_matrix(G).toarray()\n",
    "    n = A.shape[0]\n",
    "\n",
    "    \n",
    "    D = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        D[i, i] = np.sum(A[i])\n",
    "    \n",
    "    \n",
    "    if eig_type == \"normal\":\n",
    "        L = np.identity(n)\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if D[i, i] != 0 and D[j, j] != 0:\n",
    "                    L[i, j] -= A[i, j] / (np.sqrt(D[i, i]) * np.sqrt(D[j, j]))\n",
    "    elif eig_type == \"rw\":\n",
    "        L = np.identity(n)\n",
    "        for i in range(n):\n",
    "            if D[i, i] != 0:\n",
    "                for j in range(n):\n",
    "                    L[i, j] -= A[i, j] / D[i, i]\n",
    "    # Copy pase from the previous exercise\n",
    "\n",
    "    #Compute eigenvalues and eigenvectors of L.\n",
    "    lambdas, eigvecs = graph_eig(L)\n",
    "    X = eigvecs[:, :d]\n",
    "    \n",
    "    \n",
    "    for i in range(n):\n",
    "        norm = np.linalg.norm(X[i])\n",
    "        if norm > 0:\n",
    "            X[i] = X[i] / norm\n",
    "\n",
    "    \n",
    "    centers, y_clust, inertia = k_means(X, n_clusters=k)\n",
    "    ### YOUR CODE HERE\n",
    "    return y_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(G, clusters):\n",
    "    fig = plt.figure(1, figsize=(20, 20), dpi=60)\n",
    "    #nx.draw(G, pos=layout,with_labels = True, node_size=50, node_color=your_clusters,font_size=8, alpha=0.2)\n",
    "    #layout = nx.spring_layout(H, k=5.15, iterations=20)\n",
    "    layout = nx.kamada_kawai_layout(G)\n",
    "    nx.draw_networkx_edges(G, layout, alpha = 0.2)\n",
    "    nx.draw_networkx_nodes(G, layout, node_color=clusters, node_size=500)\n",
    "    nx.draw_networkx_labels(G, layout)\n",
    "\n",
    "your_clusters = spect_cluster(G, k=6)   \n",
    "plot_graph(G, your_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-remove",
   "metadata": {},
   "source": [
    "### Task 2.2.3 (4 points)\n",
    "\n",
    "Finally, use your implementation of spectral clustering with different Laplacians and different values of $k \\in [3,10]$ and plot the results using the helper function ```plot_graph```. \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> the results you obtain. Especially, what is the difference between the Random Walk and the Normalized Laplacians, if any? How do you explain such differences? Can you detect easily all the ground truth communities? Are some communities not detected? Why do you think that happens? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [3, 5, 7, 10]\n",
    "\n",
    "# First: Normalized Laplacian\n",
    "for k in k_values:\n",
    "    clusters_norm = spect_cluster(G, eig_type=\"normal\", k=k, d=5)\n",
    "    print(f\"Plotting Normalized Laplacian (k={k})\")\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plot_graph(G, clusters_norm)\n",
    "    plt.title(f\"Normalized Laplacian (k={k})\")\n",
    "    plt.show()\n",
    "\n",
    "# Then: Random Walk Laplacian\n",
    "for k in k_values:\n",
    "    clusters_rw = spect_cluster(G, eig_type=\"rw\", k=k, d=5)\n",
    "    print(f\"Plotting Random Walk Laplacian (k={k})\")\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plot_graph(G, clusters_rw)\n",
    "    plt.title(f\"Random Walk Laplacian (k={k})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-reality",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "\n",
    "Well, we can definetely see that the correct clustering is when k=3, Where the Normalized Laplacian offers a better clustering representation than the Random Walk Laplacian. This is due the fact that Random Walk, in contrary to the Normalized Laplacian, have the degree information being used directly, without the square root, causing very low or high degree to have a more pronounced effect on the embedding, not a more uniform distribution as what we see in the normalizd Laplacinan. \n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-colon",
   "metadata": {},
   "source": [
    "### Task 2.2.4 (4 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the modularity. Recall that the definition of modularity for a set of communities $C$ is\n",
    "$$ \n",
    "Q=\\frac{1}{2 m} \\sum_{c \\in C} \\sum_{i \\in c} \\sum_{j \\in c}\\left(A_{i j}-\\frac{d_{i} d_{j}}{2 m}\\right) \\qquad \\qquad (1) \n",
    "$$\n",
    "where $A$ is the adjacency matrix, and $d_i$ is the degree of node $i$\n",
    "\n",
    "**Note**: Use ```plot_graph``` function in order to see for yourself if maximising modularity leads a better clustering. If you did not succeed with the previous Task you are allowed to use [Scikit Learn Spectral Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def modularity(G, clustering):\n",
    "    IDnode = pickle.load(open('./data/IDnode.pickle', 'rb'))\n",
    "\n",
    "    nodes = list(G.nodes())\n",
    "    communities = {}\n",
    "    \n",
    "    # Match clustering indices directly with nodes list (safe)\n",
    "    for node_idx, cluster_id in enumerate(clustering):\n",
    "        communities.setdefault(cluster_id, set()).add(nodes[node_idx])\n",
    "\n",
    "    m = G.size(weight=None) \n",
    "    modularity = 0\n",
    "\n",
    "    for community in communities.values():\n",
    "        for i in community:\n",
    "            for j in community:\n",
    "                A_ij = 1 if G.has_edge(i, j) else 0\n",
    "                d_i = G.degree(i)\n",
    "                d_j = G.degree(j)\n",
    "                modularity += A_ij - (d_i * d_j) / (2 * m)\n",
    "\n",
    "    modularity /= (2 * m)\n",
    "\n",
    "    return modularity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-inspiration",
   "metadata": {},
   "source": [
    "### Task 2.2.5 (3 points)\n",
    "\n",
    "Compute the modularity of your Spectral Clustering Implementation for different values of $k$. \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> which value maximises the modularity. Is $k=6$ maximizing the modularity? If yes, is this consistent with the ground-truth? If not, is it because of an issue with modularity or with spectral clustering? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "mods = []\n",
    "ks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "adj_matrix = nx.to_numpy_array(G) ##\n",
    "\n",
    "for k in ks:\n",
    "    ##clusters = spect_cluster(G, k=k) ### NOTE: If you do not use your implementation substitute with a call to the sklearn one. \n",
    "    sc = SpectralClustering(n_clusters=k, affinity='precomputed', assign_labels='kmeans', random_state=42)\n",
    "    clusters = sc.fit_predict(adj_matrix)\n",
    "    mods.append(modularity(G, clusters))\n",
    "\n",
    "# You may want to use plt.plot to plot the modularity for different values of k\n",
    "plt.plot(ks, mods)\n",
    "print(mods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-malaysia",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "\n",
    "Well, we can see that 6 is not quite maximizing the modularity, having the modularity in both my spectral clustering and the library based spectral clustering algorithm to actually have a slightly higher modularity at k=10, at 0.6, whereas the modularity 0.578 is given k=6. This is likely due to the fact that more clusters allow the algorithm to better optimize intra-community edges. The modularity tends towards smaller, tight groups, that's why it increases given k is higher. \n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-power",
   "metadata": {},
   "source": [
    "### Task 2.2.6 (2 points)\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> There seems to be a relationship between graph embeddings and spectral clustering, can you guess that? _Hint_: Think to the eigenvectors of the graph's Laplacians."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-salvation",
   "metadata": {},
   "source": [
    "* [X] If the embeddings are linear and the similarity is the Laplacian, the embeddings we obtain minimizing the $L_2$ norm are equivalent to the eigenvectors of the Laplacian. \n",
    "* [ ] If the embeddings are random-walk-based embeddings, the eigenvectors of the Random Walk Laplacian are related to the embeddings obtained by such methods. \n",
    "* [ ] The relationship is just apparent. \n",
    "* [ ] If the embeddings are linear and the similarity is the Adjacency matrix, the eigenvectors of the Laplacian are equivalent to the embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-settle",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>\n",
    "\n",
    "Well, the first answer is the correct one. This is due to the fact the minimizing of $L_2$, regarding certain constraints such as orthogonalitya nd normlization of the embedding vectors, leads to the eigenvalue problem, solution being the eigenvectors. \n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8830e2",
   "metadata": {},
   "source": [
    "# Part 3: Link analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e0131",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">In the following sections we will be using **Cora** dataset. Below you can find a brief description of the dataset. More detailed info can be found in [./data/cora/README](./data/cora/README).</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89310b",
   "metadata": {},
   "source": [
    "The **Cora dataset** is a collection of **Machine Learning papers** categorized into **seven classes**: Case-Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, and Theory. It contains **2708 papers**, where each paper cites or is cited by at least one other paper, forming a citation graph.\n",
    "\n",
    "The dataset includes:\n",
    "- **content file**: Paper descriptions with binary word attributes (presence or absence of 1433 unique words) and class labels.\n",
    "- **cites file**: Citation relationships between papers, indicating which paper cites another.\n",
    "- **papers file**: List of postscript files with IDs, filenames, and citation strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f05074",
   "metadata": {},
   "source": [
    "## Task 3.1 Cora dataset and PageRank (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62650bd",
   "metadata": {},
   "source": [
    "The code below will be useful when working on exercises in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969cf96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.read_csv('data/cora/cora.content', sep='\\t',\n",
    "                  header=None,\n",
    "                  usecols=[0, 1434],\n",
    "                  names=['node_id', 'label'],\n",
    "                  dtype={'node_id': str, 'label': 'category'},\n",
    "                  index_col='node_id')\n",
    "\n",
    "content['label_code'] = content['label'].cat.codes\n",
    "content.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dac0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.read_csv('data/cora/cora.papers', sep='\\t',\n",
    "                  header=None,\n",
    "                  names=['node_id', 'filename', 'citation'],\n",
    "                  dtype={'node_id': str},\n",
    "                  index_col='node_id')\n",
    "\n",
    "papers.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7116a",
   "metadata": {},
   "source": [
    "To get a better feeling of how the dataset looks like, you can run the code below to plot the graph with ground truth labelings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "G: nx.DiGraph = nx.read_edgelist(\"data/cora/cora.cites\", create_using=nx.DiGraph()).reverse()\n",
    "plt.figure(1, figsize=(20, 20), dpi=60)\n",
    "with open('data/cora/cora.layout.pkl', 'rb') as f:\n",
    "    layout = pickle.load(f)\n",
    "labels = content['label_code'].loc[list(G.nodes)].to_numpy()\n",
    "unique_labels = content['label'].cat.categories\n",
    "colors = plt.cm.nipy_spectral(np.linspace(0, 1, len(unique_labels), endpoint=False))\n",
    "color_map = dict(zip(unique_labels, colors))\n",
    "node_colors = [color_map[content.loc[node]['label']] for node in G.nodes]\n",
    "nx.draw_networkx_nodes(G, layout, node_color=node_colors, node_size=50)\n",
    "nx.draw_networkx_edges(G, layout, alpha = 0.2)\n",
    "\n",
    "handles = [mpatches.Patch(color=color_map[label], label=label) for label in unique_labels]\n",
    "plt.legend(handles=handles, loc='best', title='Node Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609c4b24",
   "metadata": {},
   "source": [
    "### Task 3.1.1 (2 points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> From the [MMD] book we know that\n",
    "\n",
    "*\"PageRank is a function that assigns a real number to each page in the Web [...] The intent is that the higher the PageRank of a page, the more\n",
    "important it is.\"*.\n",
    "\n",
    "Put yourself in the shoes of an eager Machine Learning student mostly interested in \"Reinforcement_Learning\" who wants to find the most important papers to read. Run **Topic-specific** PageRank on the graph induced by the Cora dataset. Find the top 10 papers with the highest PageRank.\n",
    "\n",
    "<span style='color: red'>For this task, use `nx.pagerank` which uses the Power Iteration method.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct directed graph\n",
    "G_: nx.DiGraph = nx.read_edgelist(\"data/cora/cora.cites\", create_using=nx.DiGraph()).reverse()\n",
    "\n",
    "### YOUR CODE HERE\n",
    "pagerank_scores = nx.pagerank(G_)\n",
    "rl_nodes = content[content['label'] == 'Reinforcement_Learning'].index.tolist()\n",
    "\n",
    "personalization = {node: 0 for node in G_.nodes()}\n",
    "if rl_nodes:\n",
    "    for paper in rl_nodes:\n",
    "        if paper in personalization:\n",
    "            personalization[paper] = 1.0 / len(rl_nodes)\n",
    "else:\n",
    "    print(\"Warning: No papers found with the topic 'Reinforcement_Learning'. Running standard PageRank.\")\n",
    "    personalization = None\n",
    "\n",
    "# Run Topic-Specific PageRank\n",
    "pagerank_scores = nx.pagerank(G_, personalization=personalization)\n",
    "\n",
    "sorted_pagerank = sorted(pagerank_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Get the top 10 papers\n",
    "top_10_papers = sorted_pagerank[:10]\n",
    "\n",
    "print(\"Top 10 papers with the highest Topic-Specific PageRank for 'Reinforcement_Learning':\")\n",
    "for paper_id, score in top_10_papers:\n",
    "    topic = content.loc[paper_id, 'label'] if paper_id in content.index else 'Unknown'\n",
    "    print(f\"Paper ID: {paper_id}, PageRank Score: {score:.6f}, Topic: {topic}\")\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6194f3",
   "metadata": {},
   "source": [
    "### Task 3.1.2 (3 points)\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> An important observation about the graph induced by the Cora dataset is that, at least in theory, it should be acyclic. Can you see why?\n",
    "\n",
    "Consider the PageRank-related problems discussed in one of the lectures, such as \"sink nodes,\" \"spider traps,\" and \"link farms.\" Do these problems exist in acyclic graphs? Motivate your answer.\n",
    "\n",
    "If **YES**, are these problems amplified or diminished?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d681c",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span> <br>\n",
    "\n",
    "Regarding that the graph is acyclic, it means that sink nodes is actually a problem due to the fact that you can get at a node C which doesn't have a connection to the general graph. To avoid cyclices, you cannot have a node pointing to a node that is allready being pointed or points to someone, for example A -> B -> C -> A not allowed, but A -> B -> C allowed, where here C is the sink node, no way to do anything given reaching that node. \n",
    "\n",
    "Regarding spider traps, this is no longer possible given a acyclic graph, due to the fact that the spider trap is a cycle by itself.\n",
    "\n",
    "Regarding link farms, these are nodes that link each other to continuesly boost their pagerank, creating reinforcing loops. Well, given that we have a acyclic graph, loops are not possible, therefore the general concept of link farms is also not feasible, resulting that link farms are not possible.\n",
    "\n",
    "Conclusion, only sink nodes is possible in a acyclic graph. \n",
    "\n",
    "A short answer regarding why Cora dataset is acyclic is due the fact that papers cannot cite future paper, only allready existing papers, therefore you cannot have a cycle in that regard, only a one way connection from papers citing allready existing papers and so on. \n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12075d44",
   "metadata": {},
   "source": [
    "### Task 3.1.3 (2 points)\n",
    "Run the same experiment as in task 3.1.1, but this time on the undirected version of the graph.\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> which experiment do you think finds more important papers? Explain why you think so.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct undirected graph\n",
    "G: nx.Graph = nx.read_edgelist(\"data/cora/cora.cites\", create_using=nx.Graph())\n",
    "\n",
    "### YOUR CODE HERE\n",
    "pagerank_scores = nx.pagerank(G)\n",
    "rl_nodes = content[content['label'] == 'Reinforcement_Learning'].index.tolist()\n",
    "\n",
    "personalization = {node: 0 for node in G.nodes()}\n",
    "if rl_nodes:\n",
    "    for paper in rl_nodes:\n",
    "        if paper in personalization:\n",
    "            personalization[paper] = 1.0 / len(rl_nodes)\n",
    "else:\n",
    "    print(\"Warning: No papers found with the topic 'Reinforcement_Learning'. Running standard PageRank.\")\n",
    "    personalization = None\n",
    "\n",
    "# Run Topic-Specific PageRank\n",
    "pagerank_scores = nx.pagerank(G, personalization=personalization)\n",
    "\n",
    "sorted_pagerank = sorted(pagerank_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# Get the top 10 papers\n",
    "top_10_papers = sorted_pagerank[:10]\n",
    "\n",
    "print(\"Top 10 papers with the highest Topic-Specific PageRank for 'Reinforcement_Learning':\")\n",
    "for paper_id, score in top_10_papers:\n",
    "    topic = content.loc[paper_id, 'label'] if paper_id in content.index else 'Unknown'\n",
    "    print(f\"Paper ID: {paper_id}, PageRank Score: {score:.6f}, Topic: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb8614",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span> <br>\n",
    "\n",
    "Well, put in simple terms, a directed graph indicates a clear distinction between the ones being cited and the ones who cites, wheras the top 10 from the pagerank shows the nodes that are cited the most, which that's what we want to find. Being cited by others the most means that you are likely a popular paper.\n",
    "\n",
    "Given undirected graph, there is no a clear distinciton between being cited or citing, therefore the top 10 will be a combination of being cited and citing the most, an inaccuarate populatiry index calculation. \n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0781c9",
   "metadata": {},
   "source": [
    "## Task 3.2 Approximate PageRank (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa2773",
   "metadata": {},
   "source": [
    "### Task 3.2.1 (4 points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span>  a different algorithm for computing Personalized PageRank. This algorithm runs a fixed number of iterations and uses the definition of random walks. \n",
    "At each step, the algorithm either selects a random neighbor with probability $\\alpha$ or returns to the starting node with probability $1-\\alpha$. Every time a node is visited a counter on the node is incremented by one. Initially, each counter is 0. The final ppr value is the values in the nodes divided by the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def approx_personalized_pagerank(G: nx.Graph, node, alpha = 0.85, iterations = 1000):\n",
    "    \"\"\"\n",
    "    Compute the approximate personalized PageRank for a given node in a graph using random walks.\n",
    "\n",
    "    Parameters:\n",
    "    G (networkx.Graph): The input graph.\n",
    "    node (int or str): The starting node for the random walks.\n",
    "    alpha (float): The probability of continuing the random walk to a neighbor.\n",
    "    iterations (int): The number of steps to perform.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are nodes and values are the approximate personalized PageRank values.\n",
    "    \"\"\"\n",
    "    ppr = dict(zip(G.nodes, np.zeros(len(G.nodes))))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    start_node = node\n",
    "\n",
    "    current_node = start_node\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        if random.random() < alpha:\n",
    "            neighbors = list(G.neighbors(current_node))\n",
    "            if neighbors:\n",
    "                current_node = random.choice(neighbors)\n",
    "        else:\n",
    "            current_node = start_node\n",
    "\n",
    "        ppr[current_node] += 1 / iterations\n",
    "    \n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return ppr\n",
    "\n",
    "\n",
    "valid_node_id = content.index[0] if not content.empty else None\n",
    "\n",
    "if valid_node_id:\n",
    "    personalized_pagerank = approx_personalized_pagerank(G_, node=valid_node_id, alpha=0.85, iterations=1000)\n",
    "    ranked_ppr = sorted(personalized_pagerank.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(f\"Personalized PageRank for node '{valid_node_id}': {ranked_ppr}\")\n",
    "else:\n",
    "    print(\"Error: Could not find a valid node ID from the content DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8a7962",
   "metadata": {},
   "source": [
    "### Task 3.2.2 (4 points)\n",
    "\n",
    "Run the ```approx_personalized_pagerank``` with default $\\alpha$ and iterations $\\{10, n, 2n, 10n, 100n, 1000n\\}$ where $n$ is the number of nodes in the graph and starting node the node with the highest PageRank. Compare it to the PageRank obtained by running ```nx.pagerank(G, alpha=0.85, personalization={node_highest_pagerank: 1})```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08a73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "G: nx.Graph = nx.read_edgelist(\"data/cora/cora.cites\", create_using=nx.Graph())\n",
    "n = G.number_of_nodes()\n",
    "\n",
    "### YOUR CODE HERE\n",
    "pagerank_scores = nx.pagerank(G)\n",
    "highest_pagerank_node = max(pagerank_scores, key=pagerank_scores.get)\n",
    "\n",
    "print(f\"Node with the highest PageRank: {highest_pagerank_node}\")\n",
    "\n",
    "\n",
    "iterations_list = [10, n, 2 * n, 10 * n, 100 * n, 1000 * n]\n",
    "\n",
    "approx_ppr_results = {}\n",
    "for iterations in iterations_list:\n",
    "    approx_ppr = approx_personalized_pagerank(G, node=highest_pagerank_node, alpha=0.85, iterations=iterations)\n",
    "    sorted_approx_ppr = sorted(approx_ppr.items(), key=lambda item: item[1], reverse=True)\n",
    "    approx_ppr_results[iterations] = sorted_approx_ppr\n",
    "\n",
    "\n",
    "personalization_vector = {highest_pagerank_node: 1}\n",
    "exact_ppr = nx.pagerank(G, alpha=0.85, personalization=personalization_vector)\n",
    "sorted_exact_ppr = sorted(exact_ppr.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "\n",
    "print(\"Approx_ppr_results: \")\n",
    "for iterations, results in approx_ppr_results.items():\n",
    "    print(f\"{iterations}: {results}\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Exact_ppr_results: \")\n",
    "print(sorted_exact_ppr)\n",
    "\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879d444",
   "metadata": {},
   "source": [
    "A). <span style='color: green'>**\\[Motivate\\]**</span>  Why are the values and the top-10 nodes ranked by Approximate PPR changing so much? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db2e61",
   "metadata": {},
   "source": [
    "******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>  \n",
    "\n",
    "Is a random walk approach, where you randomly by alpha you go to a random neighbor (even random distribution of which neighbor you are going to), or jump back to start_node. Due to its randomness and the even distribution of random neighbor visit, you get the top-10 nodes changing very much per running the code multiple time.\n",
    "\n",
    "If you are refering to the difference in the iterations and why getting different top-10 given different iterations is due to the possibility of visiting the full graph or discovering more of the graph. If you have a low iteration, even if you are the most popular node and have the most amount of connecitons and possibilities to visit as many nodes as possible, you are still restricted to visit a specific amount of neighbors lower than the number of iteration (sinking then going back and because of the 1-alpha jumps). It stabilizes more when running with high number of iterations, but the probabilities can still shifting from the top-10 due to my inital argument. \n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a1759",
   "metadata": {},
   "source": [
    "B). <span style='color: green'>**\\[Motivate\\]**</span> What do you notice as the number of iterations increase? Is there a relationship between the number of iterations and the results? Is there a relationship between the approximated value of PageRank and the real value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1613a3e2",
   "metadata": {},
   "source": [
    "*****************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>  \n",
    "\n",
    "Given bigger number of iterations, we see a better stabilization in result, due to the fact that we the graph is more and more covered, until you reach a maximum point of convergence on the graph. Given low iterations, you will manage to converge only to the closer regions, locally to the initial starting node. Until you don't discover the full graph, the visited nodes will different from each intermediary iteration, due to different possible path discoveries. Given high iterations, you allready discovered the full paths, therefore smaller differences.\n",
    "\n",
    "There is a relation between the approximated value of PageRank and the real value, where if high iteration and high discovery of the other neighbors and paths, we get, at least for the top 5 nodes, very similar probabilities for the closest nodes, and beyond, we get somewhat similar approximations, where the order from highest to lowest reachable in probability can vary a little given allready low probabilities for those\n",
    "\n",
    "*****************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee972ca",
   "metadata": {},
   "source": [
    "### Task 3.2.3 (2 points)\n",
    "\n",
    "Run again the same experiment as in task 3.2.2 but this time use $\\alpha = 0.1$. \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Motivate whether and why you need more or less iterations to predict the 10 nodes with the highest PPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c192bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "G: nx.Graph = nx.read_edgelist(\"data/cora/cora.cites\", create_using=nx.Graph())\n",
    "n = G.number_of_nodes()\n",
    "\n",
    "### YOUR CODE HERE\n",
    "pagerank_scores = nx.pagerank(G)\n",
    "highest_pagerank_node = max(pagerank_scores, key=pagerank_scores.get)\n",
    "\n",
    "print(f\"Node with the highest PageRank: {highest_pagerank_node}\")\n",
    "\n",
    "\n",
    "iterations_list = [10, n, 2 * n, 10 * n, 100 * n, 1000 * n]\n",
    "\n",
    "approx_ppr_results = {}\n",
    "for iterations in iterations_list:\n",
    "    approx_ppr = approx_personalized_pagerank(G, node=highest_pagerank_node, alpha=0.85, iterations=iterations)\n",
    "    sorted_approx_ppr = sorted(approx_ppr.items(), key=lambda item: item[1], reverse=True)\n",
    "    approx_ppr_results[iterations] = sorted_approx_ppr\n",
    "\n",
    "\n",
    "personalization_vector = {highest_pagerank_node: 1}\n",
    "exact_ppr = nx.pagerank(G, alpha=0.1, personalization=personalization_vector)\n",
    "sorted_exact_ppr = sorted(exact_ppr.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "\n",
    "print(\"Approx_ppr_results: \")\n",
    "for iterations, results in approx_ppr_results.items():\n",
    "    print(f\"{iterations}: {results}\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"Exact_ppr_results: \")\n",
    "print(sorted_exact_ppr)\n",
    "\n",
    "\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792612b",
   "metadata": {},
   "source": [
    "*****************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>  \n",
    "\n",
    "Well, because alpha is 0.1, it means that with 90% probability, we jump back to the starting node, therefore the learning process is much much lower, therefore we need way more iterations to get the first top 10 accurately correct regarding the exact ppr, as well as getting a more stable result in the end. \n",
    "\n",
    "*****************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ce7d9",
   "metadata": {},
   "source": [
    "## Task 3.3 Spam and link farms (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b36ce9",
   "metadata": {},
   "source": [
    "While the Cora dataset is theoretically expected to be acyclic, this is not always the case in practice. Two papers may cite each other (e.g., preprints or conference versions), forming a cycle that artificially boosts their PageRank. Although a single cycle has a minor impact on PageRank, the effect accumulates when a node is part of many cycles. From the lectures, you should know that this type of structure is called a *link farm*.\n",
    "\n",
    "The goal of this exercise is to identify link farms in Cora graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c83cd3",
   "metadata": {},
   "source": [
    "### Task 3.3.1 (3 points)\n",
    "A). <span style='color: green'>**\\[Implement\\]**</span> <br> \n",
    "Compute spam mass of each node.\n",
    "\n",
    "***[Hint]** One way to interpret spam mass is as the deviation of TrustRank from PageRank. For more info check lecture notes.* <br>\n",
    "***[Note]** To compute PageRank you can use `nx.pagerank`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34074e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directed graph where an edge points from the paper which contains the citation to the paper being cited\n",
    "G: nx.DiGraph = nx.read_edgelist(\"data/cora/cora.cites\", create_using=nx.DiGraph()).reverse()\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# PageRank\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# TrrustRank  top 5% of nodes\n",
    "num_nodes = G.number_of_nodes()\n",
    "seed_set_size = int(0.05 * num_nodes)\n",
    "sorted_pagerank = sorted(pagerank.items(), key=lambda item: item[1], reverse=True)\n",
    "seed_nodes = [node for node, score in sorted_pagerank[:seed_set_size]]\n",
    "\n",
    "personalization_trustrank = {node: 0 for node in G.nodes()}\n",
    "if seed_nodes:\n",
    "    for node in seed_nodes:\n",
    "        if node in personalization_trustrank:\n",
    "            personalization_trustrank[node] = 1.0 / len(seed_nodes)\n",
    "\n",
    "trustrank_plus = nx.pagerank(G, personalization=personalization_trustrank)\n",
    "\n",
    "# Spam mass\n",
    "spam_mass = {}\n",
    "for node in G.nodes():\n",
    "    rz = pagerank.get(node, 0)\n",
    "    rz_plus = trustrank_plus.get(node, 0)\n",
    "    rz_minus = rz - rz_plus\n",
    "    if rz > 0:\n",
    "        spam_mass[node] = rz_minus / rz\n",
    "    else:\n",
    "        spam_mass[node] = 0.0\n",
    "\n",
    "\n",
    "print(\"Spam Mass of Each Node:\")\n",
    "for node, mass in spam_mass.items():\n",
    "    print(f\"Node: {node}, Spam Mass: {mass:.6f}\")\n",
    "\n",
    "#spam_mass = [spam_mass.get(node, 0) for node in G.nodes()]\n",
    "spam_mass = list(spam_mass.values())\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d88aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code below to visualize how the spam mass computed by you is distributed across the graph\n",
    "c_map = plt.cm.OrRd\n",
    "plt.figure(1, figsize=(16, 16), dpi=60)\n",
    "with open('data/cora/cora.layout.pkl', 'rb') as f:\n",
    "    layout = pickle.load(f)\n",
    "nx.draw_networkx_edges(G, layout, alpha = 0.2, arrows=False) # arrows=False to speed up plotting\n",
    "nx.draw_networkx_nodes(G, layout, node_color=\"black\", node_size=40)\n",
    "nx.draw_networkx_nodes(G, layout, node_color=spam_mass, node_size=40, cmap=c_map)\n",
    "rg = max(spam_mass) - min(spam_mass)#spam_mass.max() - spam_mass.min()\n",
    "\n",
    "min_val = min(spam_mass) #spam_mass.min()\n",
    "\n",
    "handles = [mpatches.Patch(color=c_map(i/10), label=f'{ (min_val +  rg*i/10):.3f}') for i in range(11)]\n",
    "plt.legend(handles=handles, loc='best', title='Spam Mass')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d9828",
   "metadata": {},
   "source": [
    "B). <span style='color: green'>**\\[Motivate\\]**</span> <br>\n",
    "Explain how you compute spam mass. How to interpret the values. Why does your method even work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd413d",
   "metadata": {},
   "source": [
    "*****************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>  \n",
    "\n",
    "I did follow the spam mass estimation formula from page 55/79 from the Link Analysis slides, where I used the Sktlearn library for the pagerank, and the trustrank being the 5% top nodes seed, being able to jump only to the 5% top nodes from any node.  Given that trustrank is based on the top 5% papers/nodes, we assume that those are that do not come from spam pages. \n",
    "\n",
    "The spam mass is then taking the difference of pagerank of the each page and then the trustrank for each of the page, dividing this with the pagerank. This calculates, for each node, the percentage of that node to be a spam. \n",
    "\n",
    "Given very low TrustRank, like 5%, we observe that the vast majority of the pages are actual spam, due to having values close to 1, whereas if higher, the values go closer to 0, indicating lower probability of being a spam. \n",
    "\n",
    "*****************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354e7cc",
   "metadata": {},
   "source": [
    "### Task 3.3.2 (3 points)\n",
    "A). <span style='color: green'>**\\[Motivate\\]**</span> <br> \n",
    "Based on the values of spam mass you computed, choose threshold value that separates spam nodes from non-spam nodes. Motivate your choice of the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc82f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE IF NEEDED\n",
    "### you might want to plot something\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28292281",
   "metadata": {},
   "source": [
    "*****************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>  \n",
    "\n",
    "*****************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42a10e",
   "metadata": {},
   "source": [
    "B). <span style='color: green'>**\\[Implement\\]**</span> <br> \n",
    "Based on the threshold value find spam nodes. Print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "# spam_nodes = \n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea01d7d",
   "metadata": {},
   "source": [
    "C). <span style='color: green'>**\\[Motivate\\]**</span> <br> \n",
    "The code below plots the spam nodes identified by you and the citation cycles present in the graph.\n",
    "\n",
    "Analyze the plot. Is there a relationship between position of spam nodes and citation cycles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spam nodes and citation cycles\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "with open('data/cora/cora.layout.pkl', 'rb') as f:\n",
    "    pos = pickle.load(f)\n",
    "nx.draw_networkx_edges(G, pos, alpha = 0.1, arrows=False)\n",
    "\n",
    "# draw cycles\n",
    "cycles = [cycle for cycle in nx.simple_cycles(G)]\n",
    "for cycle in cycles:\n",
    "    cycle_edges = list(zip(cycle, cycle[1:] + [cycle[0]]))\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=cycle_edges, edge_color='red', alpha=0.2, arrows=False)\n",
    "\n",
    "# draw spam nodes\n",
    "nx.draw_networkx_nodes(G, pos, nodelist=spam_nodes, node_color=\"blue\", node_size=20)\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Spam Node')\n",
    "red_patch = mpatches.Patch(color='red', label='Citation Cycle')\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29511b",
   "metadata": {},
   "source": [
    "*****************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span>  \n",
    "\n",
    "*****************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1af762",
   "metadata": {},
   "source": [
    "# Part 4: Graph embeddings\n",
    "\n",
    "In this section we will be using Cora dataset. Basic information about the dataset can be found at the beginning of [section 3](#part-3-link-analysis).\n",
    "\n",
    "The goal is, given the set of papers and all information we have about them, we want to find groups of similar papers. We know ground truth labeling of the papers, we will use it to measure the performance of our methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The graph of papers\n",
    "G: nx.Graph = nx.read_edgelist(\"data/cora/cora.cites\", create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The information we have about the papers, including ground truth labeling\n",
    "content = pd.read_csv('data/cora/cora.content', sep='\\t', header=None, dtype={0: str, 1434: 'category'})\n",
    "\n",
    "content.rename(columns={0: 'node_id'}, inplace=True)\n",
    "content.rename(columns={1434: 'label'}, inplace=True)\n",
    "content['label_code'] = content['label'].cat.codes\n",
    "content.set_index('node_id', inplace=True)\n",
    "\n",
    "content.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb5d08",
   "metadata": {},
   "source": [
    "***[Hint]**<br>\n",
    "To get adjacency matrix use `nx.to_numpy_array(G)` <br>\n",
    "To get vector of labels use `content['label_code'].loc[list(G.nodes)].to_numpy()` <br>\n",
    "To get matrix of word attributes use `content[range(1, 1434)].loc[list(G.nodes)].to_numpy()`*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d0970",
   "metadata": {},
   "source": [
    "## Task 4.1 (13 points)\n",
    "\n",
    "In this part, the strategy is going to be the following:\n",
    "\n",
    "1. Use VERSE [[1]](https://arxiv.org/pdf/1803.04742.pdf) to produce embeddings of the nodes in the graph.\n",
    "2. Use K-Means to cluster the embeddings. Measure and report NMI for the clustering. \n",
    "\n",
    "[[1](https://arxiv.org/pdf/1803.04742.pdf)] Tsitsulin, A., Mottin, D., Karras, P. and Mller, E., 2018, April. Verse: Versatile graph embeddings from similarity measures. In Proceedings of the 2018 World Wide Web Conference (pp. 539-548)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6b21d7",
   "metadata": {},
   "source": [
    "### Task 4.1.1 (6 points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the methods below to compute sampling version of VERSE. \n",
    "\n",
    "_Hint1:_ it might be a help to look in the original article \\[1\\] above.\n",
    "\n",
    "_Hint2:_ Line 14-15 in the pseudo code from the paper contains a typo:<br>\n",
    " Line 14 should be $W_u \\leftarrow W_u+(g*W_v)$\n",
    "\n",
    " Line 15 should be $W_v \\leftarrow W_v+(g*W_u)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8298ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "def sigmoid(x):\n",
    "    ''' sigmoid of x '''\n",
    "    ### YOUR CODE HERE\n",
    "    clipped_x = np.clip(x, -500, 500)\n",
    "    result = 1 / (1 + np.exp(-clipped_x))\n",
    "    ### YOUR CODE HERE\n",
    "    return result\n",
    "\n",
    "def pagerank_matrix(G: Union[nx.Graph, nx.DiGraph], alpha = 0.85) :\n",
    "    ''' pagerank matrix '''\n",
    "    ### YOUR CODE HERE\n",
    "    n = G.number_of_nodes()\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    P = np.zeros((n, n))\n",
    "    for i, node in enumerate(tqdm(node_list, desc=\"ppr matrix\")):\n",
    "        personalization = {n: 0 for n in node_list}\n",
    "        personalization[node] = 1\n",
    "        try:\n",
    "            ppr_vector_dict = nx.pagerank(G, alpha=alpha, personalization=personalization, max_iter=100, tol=1e-6)\n",
    "            for target_node, score in ppr_vector_dict.items():\n",
    "                if target_node in node_to_idx:\n",
    "                    j = node_to_idx[target_node]\n",
    "                    P[i, j] = score\n",
    "        except nx.PowerIterationFailedConvergence:\n",
    "            print(f\"converge fail {node}, uniform fallback {i}.\")\n",
    "            P[i, :] = 1.0 / n\n",
    "    row_sums = P.sum(axis=1)\n",
    "    non_zero_rows = row_sums > 0\n",
    "    P[non_zero_rows] = P[non_zero_rows] / row_sums[non_zero_rows, np.newaxis]\n",
    "    zero_rows = ~non_zero_rows\n",
    "    if np.any(zero_rows):\n",
    "        print(f\"zero rows {np.sum(zero_rows)}, uniform.\")\n",
    "        P[zero_rows, :] = 1.0 / n\n",
    "    ### YOUR CODE HERE\n",
    "    assert np.allclose(P.sum(axis=1), 1), \"rows sum 1\"\n",
    "    assert np.all(P >= 0), \"non-negative\"\n",
    "    return P\n",
    "\n",
    "def update(u, v, Z, C, step_size) :\n",
    "    '''update Z gradients'''\n",
    "    ### YOUR CODE HERE\n",
    "    D_label = C\n",
    "    n_nodes = Z.shape[0]\n",
    "    if not (0 <= u < n_nodes and 0 <= v < n_nodes):\n",
    "        print(f\"invalid index u={u}, v={v} for {Z.shape}\")\n",
    "        return\n",
    "    W_u = Z[u, :]\n",
    "    W_v = Z[v, :]\n",
    "    dot_product = np.dot(W_u, W_v)\n",
    "    q = (D_label - sigmoid(dot_product)) * step_size\n",
    "    grad_W_u = q * W_v\n",
    "    grad_W_v = q * W_u\n",
    "    Z[u, :] += grad_W_u\n",
    "    Z[v, :] += grad_W_v\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "def verse(G, S, d, k = 3, step_size = 0.05, steps = 100_000):\n",
    "    ''' sampled verse '''\n",
    "    n = G.number_of_nodes()\n",
    "    ### YOUR CODE HERE\n",
    "    variance = 1.0 / d\n",
    "    std_dev = np.sqrt(variance)\n",
    "    Z = np.random.normal(loc=0.0, scale=std_dev, size=(n, d))\n",
    "    node_indices = np.arange(n)\n",
    "    for _ in tqdm(range(steps), desc=\"verse train\"):\n",
    "        u = np.random.choice(node_indices)\n",
    "        sim_distribution = S[u, :]\n",
    "        sim_distribution = np.maximum(sim_distribution, 0)\n",
    "        prob_sum = np.sum(sim_distribution)\n",
    "        if not np.isclose(prob_sum, 1.0):\n",
    "            if prob_sum > 0:\n",
    "                sim_distribution /= prob_sum\n",
    "            else:\n",
    "                sim_distribution = np.ones(n) / n\n",
    "        try:\n",
    "            v = np.random.choice(node_indices, p=sim_distribution)\n",
    "        except ValueError as e:\n",
    "            print(f\"sample error {u}: {e}\")\n",
    "            print(f\"prob sum: {np.sum(sim_distribution)}\")\n",
    "            continue\n",
    "        update(u, v, Z, 1, step_size)\n",
    "        for _ in range(k):\n",
    "            v_bar = np.random.choice(node_indices)\n",
    "            while v_bar == v:\n",
    "                v_bar = np.random.choice(node_indices)\n",
    "            update(u, v_bar, Z, 0, step_size)\n",
    "    ### YOUR CODE HERE\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use already generated embeddings\n",
    "load_embeddings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd36958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code runs the `verse` algorithm above on G. You might want to change `step_size` and `steps` \n",
    "if not load_embeddings:\n",
    "    P   = pagerank_matrix(G)\n",
    "    emb = verse(G, P, 8, step_size=0.08, steps=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aeb570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can save the embeddings to a local file using the below\n",
    "if not load_embeddings:\n",
    "    np.save('verse.npy', emb)\n",
    "else:\n",
    "    emb = np.load('verse.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18de65d",
   "metadata": {},
   "source": [
    "### Task 4.1.2 (3 points)\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> a small piece of code that runs K-means on the embeddings with $k \\in [2,10]$ to evaluate the performance compared to Spectral clustering from section 2 using the NMI as measure. You can use ```sklearn.metrics.normalized_mutual_info_score``` for the NMI and ```sklearn.cluster.KMeans``` for kmeans. In both cases, you can use your own implementation from Handin 1 or the exercises, but it will not give you extra points.  \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> which of the method performs the best and whether the results show similarities between the two methods\n",
    "\n",
    "*[Note] You might need to tweak parameters of VERSE and Spectral Clustering to get meaningful results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "node_list_ordered = list(G.nodes())\n",
    "ground_truth_labels = content.loc[node_list_ordered]['label_code'].to_numpy()\n",
    "\n",
    "k_values = range(2, 11) # k is [2, 10]\n",
    "nmi_scores_verse = []\n",
    "\n",
    "print(\"Running K-Means on VERSE embeddings...\")\n",
    "for k in tqdm(k_values, desc=\"K-Means Progress\"):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=50).fit(emb) # random state set for determinism\n",
    "    nmi = normalized_mutual_info_score(ground_truth_labels, kmeans.labels_)\n",
    "    nmi_scores_verse.append(nmi)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, nmi_scores_verse, marker='o', label='VERSE + K-Means NMI')\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"Normalized Mutual Information (NMI)\")\n",
    "plt.title(\"NMI Score vs. Number of Clusters (VERSE Embeddings)\")\n",
    "plt.xticks(k_values)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNMI Scores (VERSE + K-Means):\")\n",
    "best_k_verse = -1\n",
    "best_nmi_verse = -1\n",
    "for k, score in zip(k_values, nmi_scores_verse):\n",
    "    print(f\"k={k}: {score:.4f}\")\n",
    "    if score > best_nmi_verse:\n",
    "        best_nmi_verse = score\n",
    "        best_k_verse = k\n",
    "print(f\"\\nBest NMI score: {best_nmi_verse:.4f} at k={best_k_verse}\")\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d301a54",
   "metadata": {},
   "source": [
    "*******************\n",
    "\n",
    "Observing the output NMI scores for k=2 through 10 allows us to identify the value of k where K-Means, using the VERSE embeddings, best aligns with the ground truth labels. It reaches its peak score at the true cluster count, suggesting, that VERSE successfully interpreted the dataset. The decrease in NMI for k > ~7 is expected, as K-Means starts unnecessarily splitting the natural clusters found in the embeddings, thus reducing the alignment with the true clustering.\n",
    "\n",
    "# TODO: add spectral clustering comparison\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87bece5",
   "metadata": {},
   "source": [
    "### Task 4.1.3 (4 points)\n",
    "A). <span style='color: green'>**\\[Implement\\]**</span> <br>\n",
    "Each node in Cora dataset has a set of binary word attributes which represent a presence or absence of 1433 unique words. Combine them with the embeddings obtained in task 4.1.1. Run K-means on data containing:\n",
    "- only embeddings from task 4.1.1\n",
    "- binary word attributes and embeddings combined\n",
    "\n",
    "Run similar experiment as in task 4.1.2, but this time compare NMI's of the above data representations.\n",
    "\n",
    "***[Hint]** To make the experiment more stable, run it, for example, 10 times and take the average of the results.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# variables\n",
    "if 'emb' not in locals():\n",
    "    emb = np.load('verse.npy')\n",
    "\n",
    "# implementation\n",
    "word_attributes = content.loc[node_list_ordered, range(1, 1434)].to_numpy()\n",
    "\n",
    "# combine embeddings and word attributes\n",
    "scaler_emb = StandardScaler()\n",
    "emb_scaled = scaler_emb.fit_transform(emb)\n",
    "\n",
    "scaler_attr = StandardScaler()\n",
    "word_attributes_scaled = scaler_attr.fit_transform(word_attributes.astype(float) + 1e-9)\n",
    "\n",
    "combined_features = np.concatenate((emb_scaled, word_attributes_scaled), axis=1)\n",
    "\n",
    "# run k-means 10 times\n",
    "n_runs = 10\n",
    "k_values = range(2, 11)\n",
    "avg_nmi_emb_only = []\n",
    "avg_nmi_combined = []\n",
    "\n",
    "print(f\"\\nrunning k-means {n_runs} times...\")\n",
    "\n",
    "for k in tqdm(k_values, desc=\"k-means comparison\"):\n",
    "    nmi_runs_emb = []\n",
    "    nmi_runs_combined = []\n",
    "    for i in range(n_runs):\n",
    "\n",
    "        # deterministic random state, but different for each iteration\n",
    "        random_state = i + 42\n",
    "\n",
    "        # k-means using original embeddings emb\n",
    "        kmeans_emb = KMeans(n_clusters=k, random_state=random_state, n_init=1).fit(emb) # Use random_state=i\n",
    "        nmi_emb = normalized_mutual_info_score(ground_truth_labels, kmeans_emb.labels_)\n",
    "        nmi_runs_emb.append(nmi_emb)\n",
    "\n",
    "        # k-means using combined_features\n",
    "        kmeans_combined = KMeans(n_clusters=k, random_state=random_state, n_init=1).fit(combined_features)\n",
    "        nmi_combined = normalized_mutual_info_score(ground_truth_labels, kmeans_combined.labels_)\n",
    "        nmi_runs_combined.append(nmi_combined)\n",
    "\n",
    "    avg_nmi_emb_only.append(np.mean(nmi_runs_emb))\n",
    "    avg_nmi_combined.append(np.mean(nmi_runs_combined))\n",
    "\n",
    "# Plotting the comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, avg_nmi_emb_only, marker='o', linestyle='-', label='avg nmi (embeddings only)')\n",
    "plt.plot(k_values, avg_nmi_combined, marker='s', linestyle='--', label='avg nmi (combined features)')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"avg nmi\")\n",
    "plt.title(\"k-means - embeddings vs combined features\")\n",
    "plt.xticks(k_values)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\naverage nmi scores:\")\n",
    "print(\"k\\temb only\\tcombined\")\n",
    "for k, nmi_e, nmi_c in zip(k_values, avg_nmi_emb_only, avg_nmi_combined):\n",
    "    print(f\"{k}\\t{nmi_e:.4f}\\t\\t{nmi_c:.4f}\")\n",
    "\n",
    "# Find best k for each representation\n",
    "best_k_emb_only = k_values[np.argmax(avg_nmi_emb_only)]\n",
    "best_nmi_emb_only = max(avg_nmi_emb_only)\n",
    "best_k_combined = k_values[np.argmax(avg_nmi_combined)]\n",
    "best_nmi_combined = max(avg_nmi_combined)\n",
    "\n",
    "print(f\"\\nbest avg nmi on emb: {best_nmi_emb_only:.4f} at k={best_k_emb_only}\")\n",
    "print(f\"best avg nmi on combined: {best_nmi_combined:.4f} at k={best_k_combined}\")\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb36fc",
   "metadata": {},
   "source": [
    "B). <span style='color: green'>**\\[Motivate\\]**</span> <br>\n",
    "Analyze the plot above. What are your conclusions? Was combining embeddings and word attributes a good idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e1e95",
   "metadata": {},
   "source": [
    "*******************\n",
    "The blue line (embeddings only) reaches its peak from 7 to 8, however the orange line (with combined features) pales in performance in comparison. Combining the VERSE embeddings with the high-dimensional binary word attributes degraded the clustering performance of K-Means. The structure captured effectively by the embeddings alone seems to be obscured or overwhelmed when the word attributes are added.\n",
    "\n",
    "Based on these results, combining the features using simple concatenation and scaling made the results worse. The embeddings alone provided a much better representation for this specific clustering task and algorithm.\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce811f",
   "metadata": {},
   "source": [
    "C). <span style='color: green'>**\\[Motivate\\]**</span> <br>\n",
    "\n",
    "Is K-means a good choice for clustering binary word attributes and embeddings combined?\n",
    "\n",
    "Consider other clustering algorithms you learned about in the first part of the course. Which one of them do you think would do better job at clustering binary word attributes and embeddings combined? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfb2e05",
   "metadata": {},
   "source": [
    "*******************\n",
    "It is not a good choice. K-Means relies on Euclidean distance and aims to minimize variance, performing best on well-separated, spherical clusters. The combined dataset merges low-dimensional, dense embeddings with very high-dimensional (1433 dimensions), sparse, binary word attributes.\n",
    "\n",
    "Euclidean distance might not effectively capture similarity in the high-dimensional binary space or the combined space. The large number of binary dimensions could dominate the distance calculation, overshadowing the potentially more informative (but lower-dimensional) embeddings, even with scaling.\n",
    "The underlying cluster shapes in the combined space are unlikely to be spherical.\n",
    "\n",
    "Methods like k-Prototypes are explicitly designed for datasets with both numerical embeddings and binary attributes, using different distance metrics appropriately. By using appropriate distance measures for each data type, k-Prototypes avoids the issues of Euclidean distance applied to binary data and the potential dominance of one feature type over the other. It respects the nature of both the embeddings and the word attributes.\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4599d",
   "metadata": {},
   "source": [
    "D). <span style='color: green'>**\\[Motivate\\]**</span> <br>\n",
    "How would you expand the graph embeddings for a multi-edge-label graph, where each edge must have exactly one label from multiple possible labels? The graph can also contain multiple edges between the same pair of nodes, provided they have different labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec0e0d",
   "metadata": {},
   "source": [
    "*******************\n",
    "Extending algorithms to consider edge types:\n",
    "- Modify random walk generation to consider edge types and generate type-specific walks whose results are combined.\n",
    "- For VERSE, redefine the cost function to consider edge types.\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9ce23",
   "metadata": {},
   "source": [
    "## Task 4.2 (10 points)\n",
    "In this section, you will use the PyTorch library, primarily designed for machine learning. If you have never worked with PyTorch before, the link below should help you get started:\n",
    "- https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8c533",
   "metadata": {},
   "source": [
    "### Task 4.2.1 (6 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> a new GCN that optimizes for modularity. The loss function takes in input a matrix $C \\in \\mathbb{R}^{n\\times k}$ of embeddings for each of the nodes. \n",
    "$C$ represents the community assignment matrix, i.e. each entry $C_{ij}$ contains the probability that node $i$ belong to community $j$. \n",
    "\n",
    "The loss function is the following\n",
    "$$\n",
    "loss = - Tr(C^\\top B C) + l\\|C\\|_2\n",
    "$$ \n",
    "where $B$ is the modularity matrix that you will also implement, and $l$ is a regularization factor controlling the impact of the $L_2$ regularizer. \n",
    "We will implement a two-layer GCN similar to the one implemented in the exercises, but the last layer's activation fucntion is a Softmax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix\n",
    "G: nx.Graph = nx.read_edgelist(\"data/cora/cora.cites\", create_using=nx.Graph())\n",
    "A           = nx.to_numpy_array(G)\n",
    "I           = np.eye(A.shape[0])\n",
    "A           = A + I # Add self loop\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Degree matrix\n",
    "row_sum = np.array(A.sum(axis=1))\n",
    "D = np.diag(row_sum.flatten())\n",
    "# D_hat^(-1/2)\n",
    "d_inv_sqrt = np.power(row_sum.flatten(), -0.5)\n",
    "# d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "# diagonal matrix D_hat^(-1/2)\n",
    "D_hat_inv_sqrt = np.diag(d_inv_sqrt)\n",
    "### YOUR CODE HERE\n",
    "\n",
    "# Normalized Laplacian\n",
    "L = D_hat_inv_sqrt @ A @ D_hat_inv_sqrt\n",
    "\n",
    "# Create input matrix\n",
    "# (keep in mind that you have access to word attributes, you might want to use it somehow)\n",
    "X = content.reindex(node_list_ordered).loc[:, list(range(1, 1434))].fillna(0).to_numpy(dtype=np.float32)\n",
    "\n",
    "### TODO your code here\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float, requires_grad=True)\n",
    "As = torch.tensor(A, dtype=torch.float)\n",
    "L = torch.tensor(L, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d5cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a GCN\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, L, input_features, output_features, activation=F.relu):\n",
    "        \"\"\"\n",
    "            Inputs:\n",
    "                L:               The \"Laplacian\" of the graph, as defined above\n",
    "                input_features:  The size of the input embedding\n",
    "                output_features: The size of the output embedding \n",
    "                activation:      Activation function sigma\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ### TODO Your code here\n",
    "        self.L = L\n",
    "        self.activation = activation\n",
    "        # weights\n",
    "        self.W = nn.Parameter(torch.empty(input_features, output_features))\n",
    "        if activation == F.relu:\n",
    "            gain = nn.init.calculate_gain('relu')\n",
    "        elif activation == F.softmax or activation == torch.sigmoid: # Softmax often uses sigmoid gain too\n",
    "            gain = nn.init.calculate_gain('sigmoid')\n",
    "        else:\n",
    "            gain = nn.init.calculate_gain('linear')\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=gain)\n",
    "        ### TODO Your code here\n",
    "     \n",
    "    def forward(self, X):\n",
    "        ### TODO Your code here\n",
    "        \n",
    "        # TODO: Ensure L is on the same device as X\n",
    "        # L_device = self.L.to(X.device)\n",
    "        \n",
    "        support = torch.mm(self.L, X) # L * X\n",
    "        X = torch.mm(support, self.W) # (L * X) * W\n",
    "        \n",
    "        # Apply the activation function if one is specified\n",
    "        if self.activation is not None:\n",
    "             # Handle softmax specifically if it were used directly here (though typically applied after)\n",
    "             if self.activation == F.softmax:\n",
    "                 X = self.activation(X, dim=1) # Apply softmax across feature dimension\n",
    "             else:\n",
    "                 X = self.activation(X) # Apply other activations like ReLU\n",
    "        ### TODO Your code here\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f0ae9",
   "metadata": {},
   "source": [
    "Define the modularity matrix and the modularity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e312591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modularity_matrix(A):\n",
    "    B = None\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    n = A.shape[0]\n",
    "\n",
    "    # local identity matrix with shape of A\n",
    "    I_local = np.eye(n)\n",
    "    A_orig = A - I_local\n",
    "\n",
    "    k = A_orig.sum(axis=1) # sum across rows to get degrees\n",
    "\n",
    "    # (k * k^T)\n",
    "    kkT = np.outer(k, k)\n",
    "    # B = A_orig - kkT / 2m\n",
    "    B = A_orig - (kkT / k.sum())\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return torch.tensor(B, dtype=torch.float)\n",
    "\n",
    "def modularity_loss(C, B, l = 0.01): \n",
    "    ''' Return the modularity loss\n",
    "\n",
    "        Args:\n",
    "            C: the node-community affinity matrix\n",
    "            B: the modularity matrix\n",
    "            l: the regularization factor\n",
    "            \n",
    "        :return the modularity loss as described at the beginning of the exercise\n",
    "    '''\n",
    "    loss = 0\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # -Tr(C^T * B * C)\n",
    "    C_transpose = C.T                       # transpose C: K x N\n",
    "    term1 = torch.mm(C_transpose, B)        # (K x N) @ (N x N) = K x N\n",
    "    term2 = torch.mm(term1, C)              # (K x N) @ (N x K) = K x K\n",
    "    modularity_score = torch.trace(term2)\n",
    "\n",
    "    # lambda * ||C||_2 (Spectral Norm)\n",
    "    spectral_norm_C = torch.linalg.norm(C, ord=2)\n",
    "    regularization_penalty = l * spectral_norm_C\n",
    "\n",
    "    # loss = -modularity + regularization\n",
    "    loss = -modularity_score + regularization_penalty\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e95b5",
   "metadata": {},
   "source": [
    "Create vector of ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d8744",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create vector of ground truth labels\n",
    "### YOUR CODE HERE\n",
    "labels = ground_truth_labels\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e544a9",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd3975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "### Encode the labels with one-hot encoding\n",
    "def to_categorical(y):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    num_classes = np.unique(y).size\n",
    "    return np.eye(num_classes, dtype='uint8')[y]\n",
    "\n",
    "def encode_label(labels):\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    return labels, label_encoder.classes_\n",
    "\n",
    "y, classes = encode_label(labels)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "# Define convolutional network\n",
    "in_features, out_features = X.shape[1], classes.size # output features as many as the number of classes\n",
    "hidden_dim = 16\n",
    "\n",
    "# Stack two GCN layers as our model\n",
    "# nn.Sequential is an implicit nn.Module, which uses the layers in given order as the forward pass\n",
    "gcn = nn.Sequential(\n",
    "    GCNLayer(L, in_features, hidden_dim),\n",
    "    GCNLayer(L, hidden_dim, out_features, None),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "gcn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd2a11",
   "metadata": {},
   "source": [
    "Train the unsupervised model. Find the best hyperparameters. You might also change the optimizer. You should be able to get at least $\\text{NMI}\\approx0.25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13830d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 100\n",
    "epochs = 1000\n",
    "\n",
    "def train_model(model, optimizer, X, B, epochs=100, print_every=10, batch_size = 2):\n",
    "    for epoch in range(epochs+1):\n",
    "        y_pred = model(X)\n",
    "        loss = modularity_loss(y_pred, B, l=l)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f'Epoch {epoch:2d}, loss={loss.item():.5f}')\n",
    "\n",
    "B = modularity_matrix(A)\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=0.01)\n",
    "train_model(gcn, optimizer, X, B, epochs=epochs, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you are done, save the model.\n",
    "torch.save(gcn, \"gcn.pt\")\n",
    "\n",
    "# # You can load it later using the code below\n",
    "# gcn = torch.load(\"gcn.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721eb069",
   "metadata": {},
   "source": [
    "Run the code below to plot the graph and the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(G, clusters):\n",
    "    fig = plt.figure(1, figsize=(20, 20), dpi=60)\n",
    "    with open('data/cora/cora.layout.pkl', 'rb') as f:\n",
    "        layout = pickle.load(f)\n",
    "    #layout = nx.kamada_kawai_layout(G)\n",
    "    nx.draw_networkx_edges(G, layout, alpha = 0.2)\n",
    "    nx.draw_networkx_nodes(G, layout, node_color=clusters, node_size=50)\n",
    "\n",
    "gcn_ = gcn.eval()\n",
    "y_pred = gcn_(X)\n",
    "max_y_pred = np.argmax(y_pred.detach().numpy(), axis=1)\n",
    "plot_graph(G, max_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe01160-1d41-4600-9794-9275f37e7ffb",
   "metadata": {},
   "source": [
    "### Task 4.2.2 (4 points)\n",
    "\n",
    "a) <span style='color: green'>**\\[Motivate\\]**</span> What are advantages and disadvantages of using a modularity-based GNN instead of spectral clustering?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0b809-31a0-479e-888b-c0ce6709de19",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span> <br>\n",
    "Modularity GNN:\n",
    "- Pro: Directly targets modularity maximization, easily incorporates node features (like word attributes) for more context.\n",
    "- Con: Can get stuck in suboptimal solutions (local optima), avoiding that requires substantial training time.\n",
    "\n",
    "Spectral Clustering:\n",
    "- Pro: good at capturing global graph structure, simpler setup without complex training loops.\n",
    "- Con: Doesn't directly optimize modularity, difficult to integrate node features beyond initial graph construction, eigenvector computation can be slow for large graphs.\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c6505d-f362-42ef-9f14-fa14ea3e0eb0",
   "metadata": {},
   "source": [
    "b) <span style='color: green'>**\\[Motivate\\]**</span> Why do we need to regularize the objective using $l\\|C\\|_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89487c5c-7ee7-4f97-9478-f0d539dbfb16",
   "metadata": {},
   "source": [
    "*******************\n",
    "<span style=\"color:red\">**YOUR ANSWER HERE**</span> <br>\n",
    "The loss function is $-Tr(C^T B C) + l * ||C||_2$. We minimize this loss.  \n",
    "The l * ||C||_2 term adds a penalty based on the magnitude of the community assignment matrix C.\n",
    "This penalty prevents the model from making the values in C excessively large just to maximize the modularity term, which could lead to numerical instability or trivial solutions. It encourages smoother, more stable community assignments.\n",
    "******************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
